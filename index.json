[{"content":"Switches or \u0026ldquo;The floor concierge\u0026rdquo; Imagine that you want to send a package from room 69 to room 62. In a typical building you cannot go to your neighbor and give them the package, it’s rude, you need to do it through the floor concierge.\nThe concierge or switch has a table of everyone\u0026rsquo;s door numbers:\n| Floor | Room number | Door number | |-------|-------------|-------------| | 6 | 602 | 1 | | 6 | 609 | 1 | Remember that each room can have many doors.\nIn reality the switch has a table that looks like this:\n| Vlan | MAC Address | Port | |------|-------------------|-------| | 6 | aa:aa:aa:aa:aa:aa | fa0/2 | | 6 | ff:ff:ff:ff:ff:ff | fa0/3 | The concierge knows to which door exactly to deliver the package.\nIn other words, a switch connects computers in a network.\nNow, the concierge\u0026rsquo;s capacity is limited by several factors:\nSize of the sending door - speed of your interface Size of the receiving door - speed of their interface Size of the hallway in which the package is moving - \u0026ldquo;medium speed\u0026rdquo; (cable, wireless, etc.) Size of the concierge desk - switch port speed Your concierge (switch) have multple roles as well.\nPackage monitoring - Traffic monitoring Package priority - QoS Bundle many doors for improved speed or redundancy - link aggregation Block packages from unwanted rooms - MAC filtering or port disabling Door monitoring - SNMP And many more But, what if you want to send your package to your neighbor in the 2nd floor?\nEnter the\u0026hellip;\nRouters or \u0026ldquo;The building concierge\u0026rdquo; This building concierge or router is the one handling the packages from one floor to another.\nIn other words, a router connects many networks together.\nSame as the floor concierge, it has a table of rooms, but it uses the room number rather than the door number.\n| Floor destination | Room quantity | Elevator door number | Room number | Notes | |:-----------------:|:-------------:|:--------------------:|:-----------:|------------------| | 1 | 256 | 255 | 1 | Easy to deliver | | 2 | 256 | 255 | 2 | Deliver at night | This table will translate to something like this:\n| Network destination | Netmask | Gateway | Interface | Metric | |:-------------------:|:---------------:|:-----------:|:-----------:|--------| | 192.168.1.1 | 255.255.255.255 | 192.168.1.255 | 192.168.1.1 | 1 | | 192.168.2.2 | 255.255.255.255 | 192.168.2.255 | 192.168.2.2 | 10 | The building concierge (router) has an entry for notes (metrics) to decide which route or time is best to deliver the package to its destination.\nBut it cannot do it alone, it needs the floors to be connected somehow\u0026hellip; like with an elevator or gateway\nThis elevator behaves like a normal room, in the sense that it has a door, but it can move from floor to floor, which means that it has a door \u0026ldquo;assigned\u0026rdquo; in floor 1 and 2.\nThis door is managed by the building concierge (router) not the floor concierge (switch). When a package needs to leave the floor, the floor concierge sends the package to the building concierge and it deliver the package to its destination.\nFrom the floor concierge (switch) perspective, is another room with a door number (MAC Address) and it can send/receive packages in the same way as if the packages were in the same floor.\nOver time, the building concierge (router) can learn new ways to deliver the packages more efficiently and work with packages based on priority or QoS.\n","permalink":"https://memo.mx/posts/understanding-computer-networks-by-analogy-part-2/","summary":"Switches or \u0026ldquo;The floor concierge\u0026rdquo; Imagine that you want to send a package from room 69 to room 62. In a typical building you cannot go to your neighbor and give them the package, it’s rude, you need to do it through the floor concierge.\nThe concierge or switch has a table of everyone\u0026rsquo;s door numbers:\n| Floor | Room number | Door number | |-------|-------------|-------------| | 6 | 602 | 1 | | 6 | 609 | 1 | Remember that each room can have many doors.","title":"Understanding Computer Networks by Analogy - Part 2 - Switches and Routers"},{"content":"Networks or \u0026ldquo;The buildings\u0026rdquo; Imagine a building with many rooms connected by hallways and staircases. This building is a lot like a computer network, where each room is a computer with its own room number called an IP address. Just like people move between rooms in a building.\nIn this building, each room can communicate directly to any room. That communication is done using network protocols. Each network protocol is like a language, some rooms communicate with others in Japanese, some other in Spanish, and so on.\nbuilding =\u0026gt; network room + room number =\u0026gt; computer + ip address language =\u0026gt; network protocol Subnets or \u0026ldquo;The floors\u0026rdquo; But what if you don\u0026rsquo;t want to communicate with everyone in the building? That\u0026rsquo;s where subnets come in. By dividing the building into floors, you can isolate communication between them. This is like creating subnetworks in a computer network, where groups of computers can communicate with each other but not with other groups.\nNow is easy to identify a room in a building by its floor and room number.\nExample room 101 can be idenfied as:\nfloor 1, room 1 Similarly, you can represent a computer by its IP address:\n192.168.1.1 Remember that this room number is unique to your building and each building can have a room 101 in its building as well.\nFor example, within your building you can communicate with room 101 using its room number, but if you want to communicate with room 101 in another building, you need to use its full address (or its public IP)\nbuilding X, floor 1, room 101 Computers or \u0026ldquo;The rooms\u0026rdquo; In reality, a room number is not tied to the physical space in that floor but to the door that allows access to it.\nA single room in your floor can have many doors, each door can have a different purpose.\n1. A main door, 2. A door for maintenance, 3. And so on. This is similar to:\n1. Wifi interface, 2. Ethernet interface, 3. And so on. Each door is identified by its name or MAC address and any message sent to that room is sent to a specific door in that room.\nIn other words, your room is identified by an door number rather than a room number within a subnet.\nroom -\u0026gt; door 1 -\u0026gt; room 101 or as a relation between Computer, MAC and IP\ncomputer interface eth0 -\u0026gt; ff:ff:ff:ff:ff:ff -\u0026gt; 192.168.1.1 ","permalink":"https://memo.mx/posts/understanding-computer-networks-by-analogy/","summary":"Networks or \u0026ldquo;The buildings\u0026rdquo; Imagine a building with many rooms connected by hallways and staircases. This building is a lot like a computer network, where each room is a computer with its own room number called an IP address. Just like people move between rooms in a building.\nIn this building, each room can communicate directly to any room. That communication is done using network protocols. Each network protocol is like a language, some rooms communicate with others in Japanese, some other in Spanish, and so on.","title":"Understanding Computer Networks by Analogy - Part 1 - Networks and subnets"},{"content":"Or is it a perspective? is it the result of the human ego? or is it just a communication problem?\nIf you look at biology, evolution has found a way to design its systems in a way that each component has a defined interface to communicate, and more importantly, each component is free to “experiment” or evolve independently from each other by random mutations.\nWhen changes in one component require a new interface it propagates those “requirements” to the other components in a trial-and-error mechanism. Thus, allowing a more dynamic evolution.\nHowever, not all good changes are permanent, and not all bad changes are discarded. The environment and the capacity to adapt are the two factor that dictates if this iteration propagates into the future.\nSo far, this has worked ok for biology.\nIn the software world, we tend to avoid nature’s trial and error approach, or at least is frowned upon, in favor of a more logical one because we have other tools at our disposal for example, we can think, we can predict, and test outcomes, and more importantly, we can work outside our brain and collaborate with other individuals to leverage other people’s experiences.\nBecause of human collaboration, complexity starts to grow. Now interfaces not only for software but for communications need to be in place. And these interfaces must be updated as fast as possible to allow a free flow of communication between everyone involved otherwise silos of information begin to form in each brain.\nWhen these interfaces are not implemented or are broken, mutations on each component start to occur and act independently from the whole system. This can lead from small failures to system wide failures or even worse, to corrupt the system into thinking it’s working as expected when in reality not.\nBut why do these human communication interfaces fail? Is it because they are difficult by the very nature of human communication? is it because a lot of context and information that is explicitly required is assumed instead?\nAlso, there is another factor that humans have but nature doesn’t… Human ego.\nWhen ego is introduced in systems it often prevents discarding what doesn’t work and those systems end up carrying a lot of garbage until that person leaves or someone else with more authority cleans it.\nThen, there is the other side of one of our tools, prediction.\nWe often try to predict how the system will behave in an unknown future that we start adding complexity to handle unknown behaviors that simply doesn’t yet exist. But why? Human ego? Or communication issue?\nAnd this leads to a cascade of decisions that now people need to think of and wrap their brains around this.\nRegardless, Complexity has consequences.\n","permalink":"https://memo.mx/posts/complexity/","summary":"Or is it a perspective? is it the result of the human ego? or is it just a communication problem?\nIf you look at biology, evolution has found a way to design its systems in a way that each component has a defined interface to communicate, and more importantly, each component is free to “experiment” or evolve independently from each other by random mutations.\nWhen changes in one component require a new interface it propagates those “requirements” to the other components in a trial-and-error mechanism.","title":"Is complexity a human construct?"},{"content":"Docker multi-stage build is a great way to build a docker image with a minimal footprint.\nCompiled languages like GoLang or Rust can take advantage of this by just copying a binary into the \u0026ldquo;deployment container\u0026rdquo;\nThis is an example from the official docs:\nFROM golang:1.16 WORKDIR /go/src/github.com/alexellis/href-counter/ RUN go get -d -v golang.org/x/net/html COPY app.go ./ RUN CGO_ENABLED=0 go build -a -installsuffix cgo -o app . FROM alpine:latest RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY --from=0 /go/src/github.com/alexellis/href-counter/app ./ CMD [\u0026#34;./app\u0026#34;] When it comes to python, most of the benefits seems to get lost. For example, is quite cumbersome to generate a binary from python code that can be shipped as-is.\nBut this won\u0026rsquo;t stop me.\nWe can use a virtualenv in the same way we would use a binary from another language.\nHere is an example:\nFROM python:3.11 as build-image WORKDIR /workspaces/modern-dev/ RUN pip install poetry \u0026amp;\u0026amp; poetry config virtualenvs.in-project true COPY poetry.lock pyproject.toml /workspaces/modern-dev/ RUN poetry install FROM python:3.11-alpine as release-image WORKDIR /workspaces/modern-dev/src COPY --from=build-image /workspaces/modern-dev/ /workspaces/modern-dev/ EXPOSE 80 COPY src/* /workspaces/modern-dev/src/ ENV PATH=\u0026#34;/workspaces/modern-dev/.venv/bin:$PATH\u0026#34; CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;app:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;80\u0026#34;] This image uses 103MB (with current poetry dependencies)\nHere is the simpler example:\nFROM python:3.11 WORKDIR /workspaces/modern-dev/ RUN pip install poetry \u0026amp;\u0026amp; poetry config virtualenvs.in-project true COPY poetry.lock pyproject.toml /workspaces/modern-dev/ RUN poetry install WORKDIR /workspaces/modern-dev/src EXPOSE 80 COPY src/* /workspaces/modern-dev/src/ ENV PATH=\u0026#34;/workspaces/modern-dev/.venv/bin:$PATH\u0026#34; CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;app:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;80\u0026#34;] With this simpler Dockerfile, the image size would be: 1.04GB, that\u0026rsquo;s 10 times bigger\n","permalink":"https://memo.mx/posts/multi-stage-build-python/","summary":"Docker multi-stage build is a great way to build a docker image with a minimal footprint.\nCompiled languages like GoLang or Rust can take advantage of this by just copying a binary into the \u0026ldquo;deployment container\u0026rdquo;\nThis is an example from the official docs:\nFROM golang:1.16 WORKDIR /go/src/github.com/alexellis/href-counter/ RUN go get -d -v golang.org/x/net/html COPY app.go ./ RUN CGO_ENABLED=0 go build -a -installsuffix cgo -o app . FROM alpine:latest RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY --from=0 /go/src/github.","title":"Multi stage docker build for python"},{"content":"Big O Notation (or the Big O) is used to describe how long and complex an operation will be based on its input.\nComplexity could mean that an operation takes N amount of time, or N amount of memory, N CPU resources, etc.\nThere are some notations to describe this:\nO(n) -\u0026gt; The complexity grows linearly based on the size of the input. O(n^2) -\u0026gt; Grows at a square ratio of its input. O(n^3) -\u0026gt; Grows at a cube ratio of its input. O(n^x) -\u0026gt; And so on. Note that the previous notations showcase that complexity always grows, at minimum as O(n). But what if the complexity grows slower than linearly?\nThis is where logarithm notations can help describe those complexities.\nBut first, what is logarithm or log?\nA logarithm is the exponent on which a number is raised, for example:\nb^p = n 2^3 = 2x2x2 2^3 = 8 In this case, p is the logarithm\nAnother example:\nlog(10)^10,000 = x 10^x = 10,000 10^4 = 10,000 log(10)^10,000 = 4 Now that we know that the log is just an exponent to raise a base (p) we can say that:\nO(log(n)) -\u0026gt; grows at a logarithmic rate based on its input. complexity described in O(log(n)) is used to define “efficient” algorithms.\nBut what all this means?\nTake binary_search for example:\nBinary Search is a searching algorithm used in a sorted array by repeatedly dividing the search interval in half. The idea of binary search is to use the information that the array is sorted and reduce the time complexity to O(Log n).\ndef binary_search(sorted_list, value): low = 0 high = len(sorted_list) - 1 while low \u0026lt;= high: mid = (low + high) // 2 if sorted_list[mid] == value: return mid elif value \u0026lt; sorted_list[mid]: high = mid - 1 else: low = mid + 1 return -1 On each iteration of this loop the input size is halved, which means that the exponent or p of this function is O(log2(n))\nTo solve a O(log(n)), take for example a list of 1024 elements and find its log.\nO(log(2)^n)) = len(sorted_list) O(log(2)^n)) = 1024 2^x = 1024 2^10 = 1024 log(2)^10) = 1024 It takes only 10 iteratios to find the value in a list of 1024 elements.\nAlso,\nO(log(log(n))) -\u0026gt; Grows at a double logarithm rate. or the complexity increases slower O(log(log(log(n)))) -\u0026gt; and so on, similar to O(n^x). ","permalink":"https://memo.mx/posts/big-o/","summary":"Big O Notation (or the Big O) is used to describe how long and complex an operation will be based on its input.\nComplexity could mean that an operation takes N amount of time, or N amount of memory, N CPU resources, etc.\nThere are some notations to describe this:\nO(n) -\u0026gt; The complexity grows linearly based on the size of the input. O(n^2) -\u0026gt; Grows at a square ratio of its input.","title":"Big O notation"},{"content":"Ambient mesh is a new data plane mode for Istio that doesn’t rely on sidecars.\nIt gives users the option to forgo sidecar proxies in favor of a mesh data plane that’s integrated into your infrastructure.\nAmbient mesh benefits are:\nMinimal configuration for traffic encryption. Same configuration for L7 policies as ”normal service mesh”. Take less resources because no sidecars are needed. Easier upgrades because pods don’t need to restart in order to upgrade the service mesh. Sidecars might break workloads (I’m looking at you GitLab…) It also gives the flexibility to opt-in on features of the service mesh according to your needs.\nNew components ztunnel Ztunnel (zero trust tunnel).\nDeployed as a daemonset in the form of a pod per node in the cluster, including Kubernetes control-plane nodes.\nIstio-CNI uses IPtables Rules to direct traffic into a tunnel (plain text for now).\nIt handles mTLS different than an Envoy proxy. An Envoy sidecar does a HTTP TLS upgrade, and it will encrypt every packet. A ztunnel encrypts every byte stream that enters into it\nFuture releases could use eBPF to route traffic to the ztunnel instead of using Iptables\nHBONE HBONE (HTTP Based Overlay Network Environment) protocol to encapsulate traffic inside the ztunnel.\nRuns on a dedicated port: 15008\nsupport metadata - \u0026lsquo;baggage\u0026rsquo; header, source/destination info\nWaypoint proxy Waypoint proxy, an Envoy proxy that handles layer 7 capabilities, deployed per namespace or per service.\nInstallation Kubernetes Kind installation Installing a 1 control-plane, 2 worker node kind kubernetes cluster\nsudo kind create cluster --config=- \u0026lt;\u0026lt;EOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: ambient nodes: - role: control-plane - role: worker - role: worker networking: apiServerAddress: \u0026#34;192.168.0.198\u0026#34; apiServerPort: 6443 EOF Increate the open file limit\nsudo sysctl fs.inotify.max_user_watches=524288 sudo sysctl fs.inotify.max_user_instances=512 Set the proper permisions to your kubeconfig\nsudo cp /root/.kube/config ~/.kube/config sudo chown $USER:$USER ~/.kube/config Istio with Ambient profile Download istioctl binary from the expermimental branch\nwget https://storage.googleapis.com/istio-build/dev/0.0.0-ambient.191fe680b52c1754ee72a06b3e0d3f9d116f2e82/istio-0.0.0-ambient.191fe680b52c1754ee72a06b3e0d3f9d116f2e82-linux-amd64.tar.gz tar -xvf istio-0.0.0-ambient.191fe680b52c1754ee72a06b3e0d3f9d116f2e82-linux-amd64.tar.gz Install\ncd istio-0.0.0-ambient.191fe680b52c1754ee72a06b3e0d3f9d116f2e82/ ./bin/istioctl install -d manifests/ --set profile=ambient -y Verify\nkubectl -n istio-system get pods NAME READY STATUS RESTARTS AGE istio-cni-node-b6t7q 1/1 Running 0 54s istio-cni-node-fblgc 1/1 Running 0 54s istio-cni-node-z2b8g 1/1 Running 0 55s istio-ingressgateway-dd667dbb7-stvfg 1/1 Running 0 55s istiod-6f9c757686-z6hq7 1/1 Running 0 2m5s ztunnel-25j69 1/1 Running 0 86s ztunnel-x5lmv 1/1 Running 0 86s ztunnel-zk2sc 1/1 Running 0 86s Observability Ambient Mesh not working properly with current tooling and dashboards\nkubectl apply -f ambient/samples/addons/prometheus.yaml kubectl apply -f ambient/samples/addons/jaeger.yaml kubectl apply -f ambient/samples/addons/kiali.yaml kubectl apply -f ambient/samples/addons/grafana.yaml Demo Deploy a demo application\nmv istio-0.0.0-ambient.191fe680b52c1754ee72a06b3e0d3f9d116f2e82 ambient kubectl apply -f ambient/samples/helloworld kubectl apply -f ambient/samples/sleep Connect to sleep pod\nkubectl exec -it sleep-78ff5975c6-s9vbf -- sh curl helloworld:5000/hello Adding workloads to the ambient mesh by adding a label to your namesapce\nistio.io/dataplane-mode=ambient\nkubectl label namespace default istio.io/dataplane-mode=ambient --overwrite=true kubectl label namespace default istio.io/dataplane-mode= --overwrite=true Waypoint proxy\napiVersion: gateway.networking.k8s.io/v1alpha2 kind: Gateway metadata: name: gateway-helloworld annotations: istio.io/service-account: sleep spec: gatewayClassName: istio-mesh kubectl apply -f manifests/waypoint.yml # kubectl delete -f manifests/waypoint.yml Waypoint policies\n# fault injection apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloworld spec: hosts: - \u0026#34;helloworld\u0026#34; http: - match: - uri: exact: /hello fault: delay: percentage: value: 100.0 fixedDelay: 5s route: - destination: host: helloworld port: number: 5000 kubectl apply -f manifests/policies.yml curl helloworld:5000/hello # kubectl delete -f manifests/policies.yml Debug export TERM=xterm-256color kubectl debug -it -n istio-system ztunnel-pl97l --image=nicolaka/netshoot termshark -i eth0 port 5000 References Kind installation Istio Ambient Mesh HBONE ","permalink":"https://memo.mx/posts/istio-ambient-mesh/","summary":"Ambient mesh is a new data plane mode for Istio that doesn’t rely on sidecars.\nIt gives users the option to forgo sidecar proxies in favor of a mesh data plane that’s integrated into your infrastructure.\nAmbient mesh benefits are:\nMinimal configuration for traffic encryption. Same configuration for L7 policies as ”normal service mesh”. Take less resources because no sidecars are needed. Easier upgrades because pods don’t need to restart in order to upgrade the service mesh.","title":"Istio Ambient Mesh"},{"content":"Update Jan 2023: Is OpenStack Still Needed in 2022? - Thierry Carrez, Open Infrastructure Foundation\nAnd why Kubernetes “won”.\nI owe my career to OpenStack and to all its contributors. I have made excellent friends, I learned a lot from them and the project itself.\nFor that and more, thanks a lot OpenStack.\nHowever…\nEven though OpenStack has never been better, I can’t shake the feeling that is fighting a lost battle.\nWhy? because it tried to replace AWS (and the rest of the cloud providers) and compete directly with them.\nThe reality is that OpenStack couldn’t do it. And had (and is) playing a catch-up game with them.\nIt not only tried to replace them by being its own cloud environment but by bringing its own APIs into the game. This is a critical part because If you take a look at Terraform, Ansible, Pulumi and others, you could see that OpenStack is an after thought for them. whereas AWS, Azure, GCP and Kubernetes are first class citizens.\nObviously, OpenStack serves specific use cases, especially when you need total control of your data and your resources and where you need total flexibility to bring your services into production.\nBut.\nHow many people “play around” with OpenStack?\nHow many people are writing the next generation services on top of it?\nHow many people are using the non-core services?\nHow many people are writing blogs about building home-labs with it?\nIs it because is “difficult” to install?\nIs that the reason why it became consulting-ware?\nOr, is it just an illusion and an echo chamber from the people I work with? Because most of them only talk about one thing… Kubernetes.\nI think the reason Kubernetes “won” the mind-share of a lot of people is because 4 things:\nThe community is pushing it and everyone wants it (even though they don’t need it). Is easy to install and experiment with. Exciting things and technologies are happening on top and alongside it. Better portability. Deploy something in your laptop, test it, change the kube-config and deploy the same thing (almost) to AWS. Also, Kubernetes did not try to replace the cloud providers, it got integrate them with them. Learning from them and evolving with them. OpenStack on the other hand was living on its own island.\nWhat could improve OpenStack’s adoption and mind-share?\nRewrite it in Rust. /s Implement transparent APIs from AWS, Azure, GCP for OpenStack, so we can reuse Pulumi and Terraform. Make it easier to install it. Make it easier to work with Prometheus stacks, Service mesh and other cloud native tools. Make it less consulting-ware… whatever that means. ","permalink":"https://memo.mx/posts/is-openstack-losing/","summary":"Update Jan 2023: Is OpenStack Still Needed in 2022? - Thierry Carrez, Open Infrastructure Foundation\nAnd why Kubernetes “won”.\nI owe my career to OpenStack and to all its contributors. I have made excellent friends, I learned a lot from them and the project itself.\nFor that and more, thanks a lot OpenStack.\nHowever…\nEven though OpenStack has never been better, I can’t shake the feeling that is fighting a lost battle.","title":"Is OpenStack fighting a lost battle?"},{"content":"TOOL Website\nTOOL - Lateralus\nBlack Then White are All I see In my infancy Red and yellow then came to be Reaching out to me Lets me see As below so above and beyond I imagine Drawn beyond the lines of reason Push the envelope Watch it bend Over thinking, over analyzing, separates the body from the mind Withering my intuition, missing opportunities and I must Feed my will to feel my moment Drawing way outside the lines Black Then White are All I see In my infancy Red and yellow then came to be Reaching out to me Lets me see There is So Much More and Beckons me To look through to these Infinite possibilities As below so above and beyond I imagine Drawn outside the lines of reason Push the envelope Watch it bend Over thinking, over analyzing, separates the body from the mind Withering my intuition, leaving opportunities behind Feed my will to feel this moment Urging me to cross the line Reaching out to embrace the random Reaching out to embrace whatever may come I embrace my desire to I embrace my desire to Feel the rhythm To feel connected Enough to step aside and Weep like a widow To feel inspired To fathom the power To witness the beauty To bathe in the fountain To swing on the spiral To swing on the spiral To swing on the spiral Of our divinity and Still be a human With my feet upon the ground I lose myself between the sounds And open wide to suck it in I feel it move across my skin I\u0026#39;m reaching up and reaching out I\u0026#39;m reaching for the random or Whatever will bewilder me Whatever will bewilder me And following our will and wind We may just go where no one\u0026#39;s been We\u0026#39;ll ride the spiral to the end And may just go where no one\u0026#39;s been Spiral out, keep going Spiral out, keep going Spiral out, keep going Spiral out, keep going ","permalink":"https://memo.mx/posts/lateralus/","summary":"TOOL Website\nTOOL - Lateralus\nBlack Then White are All I see In my infancy Red and yellow then came to be Reaching out to me Lets me see As below so above and beyond I imagine Drawn beyond the lines of reason Push the envelope Watch it bend Over thinking, over analyzing, separates the body from the mind Withering my intuition, missing opportunities and I must Feed my will to feel my moment Drawing way outside the lines Black Then White are All I see In my infancy Red and yellow then came to be Reaching out to me Lets me see There is So Much More and Beckons me To look through to these Infinite possibilities As below so above and beyond I imagine Drawn outside the lines of reason Push the envelope Watch it bend Over thinking, over analyzing, separates the body from the mind Withering my intuition, leaving opportunities behind Feed my will to feel this moment Urging me to cross the line Reaching out to embrace the random Reaching out to embrace whatever may come I embrace my desire to I embrace my desire to Feel the rhythm To feel connected Enough to step aside and Weep like a widow To feel inspired To fathom the power To witness the beauty To bathe in the fountain To swing on the spiral To swing on the spiral To swing on the spiral Of our divinity and Still be a human With my feet upon the ground I lose myself between the sounds And open wide to suck it in I feel it move across my skin I\u0026#39;m reaching up and reaching out I\u0026#39;m reaching for the random or Whatever will bewilder me Whatever will bewilder me And following our will and wind We may just go where no one\u0026#39;s been We\u0026#39;ll ride the spiral to the end And may just go where no one\u0026#39;s been Spiral out, keep going Spiral out, keep going Spiral out, keep going Spiral out, keep going ","title":"TOOL - Lateralus"},{"content":" Define the scope of the problem\nAvoid ambiguous words when defining your scope, for example, a maintainable solution … what is that? Make a solution able to adapt to changes and requirements.\nThere is no such thing as the perfect architecture, but there are definitely bad architectures. Identify them as soon as possible.\nIdentify what matters for your system.\nWhat do you care the most? performance? resiliency? consistency? something else? What matters changes from system to system.\nDon’t spend extra cost on performance (or some other metric) if that’s not required.\nIdentify the key characteristics of your system and let that guide your architecture\nAn architect should also organize teams, because a team structure should reflect the solution architecture. A system is the reflection of the organization that created it… so you must change your organization if you want something “different” or “new” ","permalink":"https://memo.mx/posts/the-role-of-a-systems-architect/","summary":"Define the scope of the problem\nAvoid ambiguous words when defining your scope, for example, a maintainable solution … what is that? Make a solution able to adapt to changes and requirements.\nThere is no such thing as the perfect architecture, but there are definitely bad architectures. Identify them as soon as possible.\nIdentify what matters for your system.\nWhat do you care the most? performance? resiliency? consistency? something else?","title":"The role of a systems architect"},{"content":"In no particular order:\nThe wailing | Gokseong\nNoroi | The Curse\nThe Medium\nSatan’s Slaves | Pengabdi Setan\nSatan\u0026rsquo;s Slaves: Communion | Pengabdi Setan 2: Communion\nKairo | Pulse\nAterrados\nIncantation | Zhou\nGonjiam: Haunted Asylum | Gon-ji-am\nA Tale Of Two Sisters | Janghwa, Hongryeon\nLake Mungo\nHereditary\nThe Shining\nThe Babadook\nVeronica\n","permalink":"https://memo.mx/posts/my-favourite-horror-movies/","summary":"In no particular order:\nThe wailing | Gokseong\nNoroi | The Curse\nThe Medium\nSatan’s Slaves | Pengabdi Setan\nSatan\u0026rsquo;s Slaves: Communion | Pengabdi Setan 2: Communion\nKairo | Pulse\nAterrados\nIncantation | Zhou\nGonjiam: Haunted Asylum | Gon-ji-am\nA Tale Of Two Sisters | Janghwa, Hongryeon\nLake Mungo\nHereditary\nThe Shining\nThe Babadook\nVeronica","title":"My favorite horror movies"},{"content":" Tools for better thinking\nThe Catalog of Design Patterns\nxmind\n","permalink":"https://memo.mx/posts/learning-resources/","summary":"Tools for better thinking\nThe Catalog of Design Patterns\nxmind","title":"Learning resources I use"},{"content":"When installing GitLab in air-gapped mode using helm charts first you need to pull the required images for the target version to your private container registry.\nBut is not straightforward to find which container tags map to which GitLab version.\nAn easy way to find which tags you need is to print the helm templates first and get the values from there.\nhelm \\ -n gitlab \\ template \\ gitlab gitlab/gitlab \\ --version 6.4.1 \\ -f values.yml \\ \u0026gt; gitlab-6.4.1.yml cat gitlab-6.4.1.yml | grep image: ","permalink":"https://memo.mx/posts/airgapped-gitlab-install/","summary":"When installing GitLab in air-gapped mode using helm charts first you need to pull the required images for the target version to your private container registry.\nBut is not straightforward to find which container tags map to which GitLab version.\nAn easy way to find which tags you need is to print the helm templates first and get the values from there.\nhelm \\ -n gitlab \\ template \\ gitlab gitlab/gitlab \\ --version 6.","title":"Installing GitLab in air-gapped mode"},{"content":"I’m trying to learn everything at once.\nI’m trying to do everything at once.\n\u0026hellip;\nExcept what I should be doing right now.\n一度にすべてを学ぼうとしています。 一度にすべてをやろうとしています。 \u0026hellip; 今すぐやるべきことを除いて。\n","permalink":"https://memo.mx/posts/busy/","summary":"I’m trying to learn everything at once.\nI’m trying to do everything at once.\n\u0026hellip;\nExcept what I should be doing right now.\n一度にすべてを学ぼうとしています。 一度にすべてをやろうとしています。 \u0026hellip; 今すぐやるべきことを除いて。","title":"Why am I always busy?"},{"content":"Kubernetes is becoming a monster and as it grows it becomes more and more challenging for newcomers to understand it.\nMy goal is to demystify its components.\nMain components There are two main components in a Kubernetes cluster.\nMaster nodes Worker nodes The only difference is the workloads they run. You can assign metadata to these nodes to schedule specific workloads on each node or type of node. For Users Containers A container is a filesystem and process wrapped in a “box” with some labels on it. Volumes A volume is a filesystem that lives outside the container. Pods Organize containers (one or more) and a volume(s) as a single unit This is the basic unit on which Kubernetes works. A pod has an IP (or more depending on your CNI) ConfigMaps Are just files that are mounted in pods (specifically, they are mounted in a container) Secrets Are just files that are mounted in the pods but the data is encoded. (specifically, they are mounted in a container). Note This is insecure, take a look at different approaches if security is important\nDeployment Is a way to tell Kubernetes the desired state of your pods, services, and volumes. It also tells Kubernetes how you want to deploy your pods, how many replicas (replicaset) or if you want a fixed amount of pods for a database cluster for example.(statefulset) Service Service has an IP that redirects traffic to your pod(s) Selectors This is how you map a service to a pod using labels Ingress Create rules to allow traffic from outside the cluster to inside the cluster Imperative vs Declarative mode You tell Kubernetes what you want to do: kubectl create ns namespace\nYou tell Kubernetes the desired state of your resource. kubectl create -f resource.yml\nFor Operators Kubernetes control plane is composed of the following components:\nkube-apiserver Runs in master nodes Creates resources in the cluster Custom resource definitions is a way to add features to the api Stores things in etcd or sqlite (for k3s) kube-scheduler Runs in the master node (a single instance at a time) It watches the kube-apiserver and schedules resources in nodes You can specify or give hints to the scheduler where you want your resources to live Custom schedulers are allowed, and more than 1 scheduler is allowed kube-controller-manager Runs in the master node (a single instance at a time) Delegates tasks to the rest of the managers to manage resources namespace-manager deployment-manager replicase-manager secret-manager operators are custom controller-managers In other words, *-managers, are a bunch of for loops watching for changes in the Kubernetes API and applying them to the cluster. They also watch for changes in the cluster against the desired state and make sure they match it. kubelet Runs in all nodes It creates containers It talks to container runtimes (rkt, docker, podman, etc.) Interacts with liveness probes Interacts with readiness probes kube-proxy Runs in all nodes It creates services and its underlying iptables anything can be extended or replaced\nKubernetes networking Pod Has a unique IP and has a CIDR Services A service has a port that listens to traffic from outside a pod(s) and sends it to the pod(s) Is an abstraction to give a name instead of an IP for other services to reach a pod(s) Types: ClusterIP The ClusterIP does not “live” in your interfaces, is an iptables rule created by kube-proxy manages this NodePort Still has a ClusterIP but also has a port in the node where the pod lives It is also an iptables rule LoadBalancer This is cloud-specific on how it works Point your client to your load balancer ip:port instead of the node_ip:port ","permalink":"https://memo.mx/posts/understanding-kubernetes/","summary":"Kubernetes is becoming a monster and as it grows it becomes more and more challenging for newcomers to understand it.\nMy goal is to demystify its components.\nMain components There are two main components in a Kubernetes cluster.\nMaster nodes Worker nodes The only difference is the workloads they run. You can assign metadata to these nodes to schedule specific workloads on each node or type of node. For Users Containers A container is a filesystem and process wrapped in a “box” with some labels on it.","title":"Demystifying Kubernetes"},{"content":"Looming recession !! , third world war !!, climate change !!\nMeanwhile taquito…\n","permalink":"https://memo.mx/posts/taquito/","summary":"Looming recession !! , third world war !!, climate change !!\nMeanwhile taquito…","title":"A lesson from my dog"},{"content":"There is a real cognitive and spiritual burden of having to carry lots of unfulfilled promises into the future.\n未来に達成されない多くの約束を抱えなければならないという現実的な認知的および精神的な負担があります。\nAt its core this is what TODOs represent to me.\n","permalink":"https://memo.mx/posts/todos/","summary":"There is a real cognitive and spiritual burden of having to carry lots of unfulfilled promises into the future.\n未来に達成されない多くの約束を抱えなければならないという現実的な認知的および精神的な負担があります。\nAt its core this is what TODOs represent to me.","title":"TODOs"},{"content":"The rapid development and adoption of cloud-native stacks that brings a better developer experience, security, reproducibility and speed at which organizations deliver value are leaving more traditional stacks behind. Hence, there is more pressure from the markets, organizations and developers to bring those stacks into a more modern era.\nWe often wonder whether the same techniques and toolchains of these modern stacks can be used to configure not-so-modern applications or infrastructures.\nThe answer is yes.\nBut how can help our organizations to adopt these modern techniques and toolchains?\nFor example, the competitive landscape of tools and services that surrounds the services that provide value has made them simple to integrate with any modern stack, more often than not with a simple click. These tools and services provide out-of-the-box functionality that can be leveraged not only for customers but your operations teams. (i.e. monitoring, tracing, backup, etc).\nThe goal of these tools is to become easy to deploy, easy to configure, pluggable, and be as self-described as possible. They do so by leveraging source control systems and CI/CD pipelines to reduce the complexity between a change being made and production deployment.\nOur goal is to leverage the same procedure.\nHowever, what happens to applications and infrastructure that weren’t designed in such a way? can they still leverage a modern approach? Yes. Using GitOps.\nGitOps is a way to implement continuous deployments toolchains for your applications/infrastructure, and it focuses on 3 main areas:\nDeveloper centric experience Infrastructure as code, store the state of your applications/infrastructure as configuration files Reconciliation loops that make the desired state of your infrastructure as code (IaaC) match what’s deployed. And that’s it! At its core, a modern GitOps toolchain consists of these 3 steps which can be easily translated to any environment.\nNow is easy to picture how can an application/infrastructure be modernized with GitOps.\nDeveloper centric experience Developers can leverage their tooling and workflows to keep pushing changes to git repositories, using all or any git workflows they are familiar with.\nInfrastructure as code Now the state of your application/infrastructure is defined as configuration and stored in git some capabilities are starting to become more accessible:\nEasy and fast error recovery, you can always redeploy to a known working state, Self-documented deployments, your configuration is your deployment, unknown states coming from manual changes are minimized, Redeployment in new environments is easier and can be automated, Immutable application/infrastructure deployments, Each commit is a deployment, each commit is value delivered. Reconciliation tools Reconciliation tools take the desired state from git and make sure it matches your environment. These tools work mainly in two paradigms:\nDeclarative infrastructure as code Imperative infrastructure as code Choosing declarative or imperative definitions for your infrastructure as code is more often than not, dictated by your organization and/or team and thus it can help you to narrow the options to choose reconciliation tools.\nFor example:\nDeclarative IaaC can use Argo CD to deploy and keep your environment synchronized in a single step using a reconciliatory loop. In this example, a tool like Argo CD will ensure that your environment is always in the desired state by observing both the desired state from your IaaC and what’s currently deployed. Imperative IaaC can use Jenkins to execute an Ansible playbook every time a commit is pushed to your git repository or periodically. Even though this is a more traditional approach, it can simulate a reconciliatory loop and give you the same result in your environment. Why GitOps is important and what value does it bring to my organization? Automation is a competency that any organization must master to bring order in this chaotic landscape. Once people, code, and tools are in place, new automation opportunities to modernize start to become more apparent to the organization. GitOps is just one of the ways to ensure control and confidence over how, when, and what you deliver.\nFAQ.\nIs my organization ready for GitOps? In short, most probably yes. Do I need specific tooling to modernize our current infrastructure? No, Using GitOps doesn’t mean using a specific set of tools, is a framework for automation best practices. ","permalink":"https://memo.mx/posts/gitops-in-enterprise/","summary":"The rapid development and adoption of cloud-native stacks that brings a better developer experience, security, reproducibility and speed at which organizations deliver value are leaving more traditional stacks behind. Hence, there is more pressure from the markets, organizations and developers to bring those stacks into a more modern era.\nWe often wonder whether the same techniques and toolchains of these modern stacks can be used to configure not-so-modern applications or infrastructures.","title":"How to use GitOps in a non-cloud-native environment"},{"content":"Be very carrefull in your setup : any misconfiguration make all the git config to fail silently !\nSetup multiple git ssh identities for git Generate your SSH keys as per your git provider documentation. Add each public SSH keys to your git providers acounts. In your ~/.ssh/config, set each ssh key for each repository as in this exemple: Host github.com HostName github.com User git IdentityFile ~/.ssh/github_private_key IdentitiesOnly=yes Host gitlab.com Hostname gitlab.com User git IdentityFile ~/.ssh/gitlab_private_key IdentitiesOnly=yes Setup dynamic git user email \u0026amp; name depending on folder Require git 2.13+ for conditional include support.\nThe idea here is to use a different git user name and email depending on the folder you are in.\nIn your ~/.gitconfig, remove the [user] block and add the following (adapt this exemple to your needs) : [includeIf \u0026#34;gitdir:~/src/personal/\u0026#34;] path = .gitconfig-personal [includeIf \u0026#34;gitdir:~/src/work/\u0026#34;] path = .gitconfig-work In your ~/.gitconfig-personal, add your personnal user informations: [user] email = user.personal@email.com name = personal_username In your ~/.gitconfig-work, add your professional user informations: [user] email = user.professional@company.com name = professional_username Setup a GPG key If you need to add a GPG key and bind it to a user to sign your commits, you can do so like this:\nYou should have GPG installed and configured like the GPG suite\nAdd the GPG key ID to your ~/.gitconfig-{PROFILE} config and enable commit signing: [user] email = your.mail@domain.com name = Your NAME signingkey = SIGNING_KEY_ID [commit] gpgsign = true Make sure to register the right GPG binary in your ~/.gitconfig: [program] pgp = /path/to/your/gpg2/bin Test your setup Now each repository will use the custom user info setup depending on the top-level folder. Check your settings are taken into account, for instance in ~/src/git/personal/ : cd ~/src/git/personal/ git config --get user.email git config --get user.name git config --get user.signingkey Do the same for each folder you have setup. You can also display and check the global git config: git config --list --global Or just the local config for the repository folder you are in: git config --list Exporting Public GPG key gpg --list-keys gpg --armor --export {KEY-ID} References Generating a new GPG key Telling Git about your signing key ","permalink":"https://memo.mx/posts/git-multiple-identities/","summary":"Be very carrefull in your setup : any misconfiguration make all the git config to fail silently !\nSetup multiple git ssh identities for git Generate your SSH keys as per your git provider documentation. Add each public SSH keys to your git providers acounts. In your ~/.ssh/config, set each ssh key for each repository as in this exemple: Host github.com HostName github.com User git IdentityFile ~/.ssh/github_private_key IdentitiesOnly=yes Host gitlab.com Hostname gitlab.","title":"Setup multiple git identities and pgp keys"},{"content":"WSL configuration Install i3\nsudo apt install i3 -y Create an init script\nvim ~/src/scripts/i3launch.sh #!/bin/zsh source ~/.zshrc # If not running interactively, don\u0026#39;t do anything [ -z \u0026#34;$PS1\u0026#34; ] \u0026amp;\u0026amp; return export DISPLAY=$(awk \u0026#39;/nameserver / {print $2; exit}\u0026#39; /etc/resolv.conf 2\u0026gt;/dev/null):0 export LIBGL_ALWAYS_INDIRECT=1 dbus_status=$(service dbus status) if [[ $dbus_status = *\u0026#34;is not running\u0026#34;* ]]; then sudo service dbus --full-restart fi i3 To run WSL2 as root\nwsl.exe -d Ubuntu-20.04 -u root -- /bin/bash Windows configuration Install vcxsrv from powershell\nwinget install vcxsrv With WSL2 you need to configure your firewall to allow WSL and vcxsrv to communicate.\nSearch for Windows Defender Firewall with Advanced Security and do the following:\nCreate an inbound rule:\nname: wsl2 rule type: port port type: tcp port number: 6000 Narrow the scope of your inbound rule:\nRight click -\u0026gt; Properties -\u0026gt; scope -\u0026gt; Remote IP addresses -\u0026gt; Add 172.16.0.0/12 Search for VcXsrv windows xserver inbound rules and make sure the 4 rules are enabled and in allow mode\nStartup i3 script from Windows vcxsrv.vbs\ncode vcxsrv.vbs Set shell = CreateObject(\u0026#34;WScript.Shell\u0026#34; ) shell.Run \u0026#34;\u0026#34;\u0026#34;C:\\Program Files\\VcXsrv\\vcxsrv.exe\u0026#34;\u0026#34; :0 -screen 0 @1 -ac -engine 1 -nodecoration -wgl\u0026#34; WScript.Sleep 200 shell.Run \u0026#34;wsl.exe -d YOUR_DISTRO -u YOUR_USER -- /bin/zsh ~/src/scripts/i3launch.sh\u0026#34;, 0 And just run your script from powershell\n.\\vcxsrv.vbs You should see this screen.\ni3 configuration Up to you.\nReferences Running i3 Desktop with WSL on Windows 10 How to set up working X11 forwarding on WSL2 WSL Windows Toolbar Launcher ","permalink":"https://memo.mx/posts/i3wm-wsl2/","summary":"WSL configuration Install i3\nsudo apt install i3 -y Create an init script\nvim ~/src/scripts/i3launch.sh #!/bin/zsh source ~/.zshrc # If not running interactively, don\u0026#39;t do anything [ -z \u0026#34;$PS1\u0026#34; ] \u0026amp;\u0026amp; return export DISPLAY=$(awk \u0026#39;/nameserver / {print $2; exit}\u0026#39; /etc/resolv.conf 2\u0026gt;/dev/null):0 export LIBGL_ALWAYS_INDIRECT=1 dbus_status=$(service dbus status) if [[ $dbus_status = *\u0026#34;is not running\u0026#34;* ]]; then sudo service dbus --full-restart fi i3 To run WSL2 as root\nwsl.exe -d Ubuntu-20.04 -u root -- /bin/bash Windows configuration Install vcxsrv from powershell","title":"i3 running on WSL2"},{"content":"Fuzzy Search documentation from the CLI.\nSee it in action here https://terminalizer.com/view/2c3935cf1418\nDisclaimer This tool was built to learn FZF capabilities. Feel free to use it or extend it.\nUsage doc-fzf ansible doc-fzf ansible -q yum Installation pip3 install doc-fzf Verify your installation:\ndoc-fzf -h usage: doc-fzf.py [-h] [-q QUERY] module_name doc-fzf. positional arguments: module_name Name of the module to search optional arguments: -h, --help show this help message and exit -q QUERY Query the docs Extending Doc-FZF doc-fzf is a modular application. It can load modules at runtime that scrap websites in any way you like.\nAny module should always contain:\nclass name must always be Screapper(FZFDoc) self.documentation_url attribute def get_documentation(self): function that must always return a tuple (\u0026ldquo;url\u0026rdquo;, \u0026ldquo;description\u0026rdquo;) from doc_fzf.pyfzf import FZFDoc class Scrapper(FZFDoc): def __init__(self): self.base_url = \u0026#34;https://docs.python.org/3\u0026#34; self.documentation_url = \u0026#34;{0}/py-modindex.html\u0026#34;.format(self.base_url) FZFDoc.__init__(self, self.documentation_url) def get_documentation(self): \u0026#34;\u0026#34;\u0026#34; Return a tuple of (url, description) \u0026#34;\u0026#34;\u0026#34; docs = get_online_documentation() for doc in docs: yield (doc.url, doc.description) Here is the ansible documentation example\nRoad Map Module definition FZFDoc base class File system cache layer Load dynamic modules References Doc-FZF: Modular CLI Documentation Fuzzy Finder fzf, A command-line fuzzy finder iterfzf, Pythonic interface to fzf, a CLI fuzzy finder ","permalink":"https://memo.mx/posts/doc-fzf/","summary":"Fuzzy Search documentation from the CLI.\nSee it in action here https://terminalizer.com/view/2c3935cf1418\nDisclaimer This tool was built to learn FZF capabilities. Feel free to use it or extend it.\nUsage doc-fzf ansible doc-fzf ansible -q yum Installation pip3 install doc-fzf Verify your installation:\ndoc-fzf -h usage: doc-fzf.py [-h] [-q QUERY] module_name doc-fzf. positional arguments: module_name Name of the module to search optional arguments: -h, --help show this help message and exit -q QUERY Query the docs Extending Doc-FZF doc-fzf is a modular application.","title":"Modular CLI Documentation Fuzzy Finder"},{"content":"Usage:\npip install pynetbox ansible ansible all -i hosts/env -m setup --tree /tmp/facts/env #!/opt/netbox/bin/python import argparse import json import os import sys import pynetbox import yaml import urllib3 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) if sys.version_info \u0026lt; (3, 6): print(\u0026#34;Python 3.6 is required\u0026#34;) sys.exit(2) def to_json(in_dict): return json.dumps(in_dict, sort_keys=True, indent=4) def load_configuration(path=\u0026#34;/etc/ansible/netbox.yml\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Load netbox configuration /etc/ansible/netbox.yml \u0026#34;\u0026#34;\u0026#34; try: with open(path, \u0026#34;r\u0026#34;) as fd: return yaml.safe_load(fd) except yaml.YAMLError as yml_error: print(yml_error) NETBOX_ENDPOINT = load_configuration()[\u0026#34;netbox_endpoint\u0026#34;] NETBOX_TOKEN = load_configuration()[\u0026#34;netbox_token\u0026#34;] if not NETBOX_ENDPOINT: raise OSError(\u0026#34;environmet var NETBOX_ENDPOINT not set\u0026#34;) if not NETBOX_TOKEN: raise OSError(\u0026#34;environmet var NETBOX_TOKEN not set\u0026#34;) nb = pynetbox.api(NETBOX_ENDPOINT, NETBOX_TOKEN, ssl_verify=False) def parse_args(): parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--list\u0026#39;, action=\u0026#39;store_true\u0026#39;) parser.add_argument(\u0026#39;--host\u0026#39;, action=\u0026#39;store\u0026#39;) parser.add_argument(\u0026#39;--introspection\u0026#39;, action=\u0026#39;store_true\u0026#39;) return parser.parse_args() def get_site(site_name): site = nb.dcim.sites.get(name=site_name) return site def get_vms(site_name): platform = nb.dcim.platforms.get(name=site_name) vms = nb.virtualization.virtual_machines.filter(platform_id=platform.id) return vms def get_devices(site_name): platform = nb.dcim.platforms.get(name=site_name) devices = nb.dcim.devices.filter(platform_id=platform.id) return devices def get_roles(): roles = nb.dcim.device_roles.all() return roles def get_role(server): try: try: return server.role except: return server.device_role except Exception as error: return \u0026#34;ungrouped\u0026#34; def which_tenant(server): try: if server.tenant: return str(server.tenant) else: # print(\u0026#34;No tenant\u0026#34;) return None except Exception as error: print(error) def which_gmn(server): try: if \u0026#34;management\u0026#34; in server.tags: return \u0026#34;management\u0026#34; else: return \u0026#34;no_management\u0026#34; except Exception as error: print(error) def get_tenants(): tenants = nb.tenancy.tenants.all() return tenants def get_device_role(site, role_name): platform = nb.dcim.platforms.get(name=site) role = nb.dcim.device_roles.get(platform_id=platform.id, name=role_name) if not role: print(\u0026#34;Role not found\u0026#34;) sys.exit(2) return role.id def get_vip(site, servers): vms = get_vms(site) proxy_vms = [] for vm in vms: if str(vm.role).lower() == \u0026#34;proxy-servers\u0026#34;: proxy_vms.append(vm) for vm in proxy_vms: ips = nb.ipam.ip_addresses.filter(virtual_machine_id=vm.id) for ip in ips: if str(ip.role) == \u0026#34;VIP\u0026#34;: return ip.address else: return \u0026#34;\u0026#34; def get_tenant_role_map(path=\u0026#34;/etc/ansible/role-tenant-map.json\u0026#34;): \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; try: with open(path, \u0026#34;r\u0026#34;) as fd: return json.load(fd) except Exception as error: print(error) def generate_inventory(site, servers): \u0026#34;\u0026#34;\u0026#34; Generate ansible groups based on roles and tenants \u0026#34;\u0026#34;\u0026#34; role_tenant_map = get_tenant_role_map() roles = get_roles() tenants = get_tenants() # base json|yaml structure inventory = { \u0026#34;all\u0026#34;: {\u0026#34;hosts\u0026#34;: []}, \u0026#34;management\u0026#34;: {\u0026#34;children\u0026#34;: {}}, \u0026#34;no_management\u0026#34;: {\u0026#34;children\u0026#34;: {}, \u0026#34;vars\u0026#34;: {}}, \u0026#34;ungrouped\u0026#34;: {\u0026#34;children\u0026#34;: {}}, \u0026#34;_meta\u0026#34;: {\u0026#34;hostvars\u0026#34;: {}} } proxy_vip = get_vip(site, servers).split(\u0026#34;/\u0026#34;)[0] if proxy_vip: inventory[\u0026#34;no_gmn\u0026#34;][\u0026#34;vars\u0026#34;] = { \u0026#34;ansible_ssh_common_args\u0026#34;: f\u0026#34;\u0026#39;-o ProxyJump={proxy_vip} -o StrictHostKeyChecking=no\u0026#39;\u0026#34; } for tenant in tenants: inventory[tenant.name] = {\u0026#34;children\u0026#34;: {}} for role in role_tenant_map[tenant.name]: inventory[tenant.name][\u0026#34;children\u0026#34;][role] = {} for role in roles: inventory[role.name] = {\u0026#34;hosts\u0026#34;: []} if role.name == \u0026#34;proxy-servers\u0026#34;: inventory[\u0026#34;management\u0026#34;][\u0026#34;children\u0026#34;][role.name] = {} else: inventory[\u0026#34;no_management\u0026#34;][\u0026#34;children\u0026#34;][role.name] = {} for server in servers: ip = str(server.primary_ip).split(\u0026#34;/\u0026#34;)[0] role = str(get_role(server)) inventory[\u0026#34;all\u0026#34;][\u0026#34;hosts\u0026#34;].append(server.name) inventory[\u0026#34;_meta\u0026#34;][\u0026#34;hostvars\u0026#34;][server.name] = { \u0026#34;ansible_host\u0026#34;: f\u0026#34;{ip}\u0026#34; } if role != \u0026#34;None\u0026#34;: inventory[role][\u0026#34;hosts\u0026#34;].append(server.name) return to_json(inventory) def get_introspection_data(site_name): \u0026#34;\u0026#34;\u0026#34; site_name: str: SITE1, SITE2, etc. return list(dicts) [ { \u0026#34;mac\u0026#34;: \u0026#34;pxe mac\u0026#34;, \u0026#34;arch\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;pm_type\u0026#34;: \u0026#34;pxe_ilo\u0026#34;, \u0026#34;pm_user\u0026#34;: \u0026#34;static per site\u0026#34;, \u0026#34;pm_password\u0026#34;: \u0026#34;static per site\u0026#34;, \u0026#34;pm_address\u0026#34;: \u0026#34;IPMI address\u0026#34; \u0026#34;name\u0026#34;: \u0026#34;position name\u0026#34; }, ] steps: query devices api on a given site (platform) for computes and controllers get pm_user and pm_password from somewhere get IPMI address \u0026#34;\u0026#34;\u0026#34; openstack_nodes = [] platform = nb.dcim.platforms.get(name=site_name) devices = nb.dcim.devices.filter(platform_id=platform.id) for device in devices: if device.custom_fields[\u0026#34;openstack_device\u0026#34;]: d = { \u0026#34;name\u0026#34;: device.name, \u0026#34;mac\u0026#34;: [device.custom_fields[\u0026#34;openstack_pxeboot_mac\u0026#34;]], \u0026#34;pm_type\u0026#34;: \u0026#34;pxe_ilo\u0026#34;, \u0026#34;arch\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;pm_user\u0026#34;: os.environ.get(\u0026#34;OS_INTROSPECTION_USER\u0026#34;, None), \u0026#34;pm_password\u0026#34;: os.environ.get(\u0026#34;OS_INTROSPECTION_PASSWORD\u0026#34;, None) } interfaces = nb.dcim.interfaces.filter(device_id=device.id) for i in interfaces: if i.name == \u0026#34;ILO\u0026#34;: ilo_ip = nb.ipam.ip_addresses.filter(interface_id=i.id)[0] d[\u0026#34;pm_address\u0026#34;] = ilo_ip.address.split(\u0026#34;/\u0026#34;)[0] openstack_nodes.append(d) return to_json(openstack_nodes) if __name__ == \u0026#34;__main__\u0026#34;: args = parse_args() # Are \u0026#34;-\u0026#34; deprecated in group names? site_name = os.environ.get(\u0026#34;SITE\u0026#34;, None) if not site_name: print(\u0026#34;Define a site to query with SITE environment variable\u0026#34;) sys.exit(2) site_name = site_name.upper() if args.introspection: introspection_data = get_introspection_data(site_name) print(introspection_data) sys.exit(0) devices = get_devices(site_name) vms = get_vms(site_name) servers = devices + vms if args.list: hosts = generate_inventory(site_name, servers) print(hosts) elif args.host: pass ","permalink":"https://memo.mx/posts/pynetbox/","summary":"Usage:\npip install pynetbox ansible ansible all -i hosts/env -m setup --tree /tmp/facts/env #!/opt/netbox/bin/python import argparse import json import os import sys import pynetbox import yaml import urllib3 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) if sys.version_info \u0026lt; (3, 6): print(\u0026#34;Python 3.6 is required\u0026#34;) sys.exit(2) def to_json(in_dict): return json.dumps(in_dict, sort_keys=True, indent=4) def load_configuration(path=\u0026#34;/etc/ansible/netbox.yml\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Load netbox configuration /etc/ansible/netbox.yml \u0026#34;\u0026#34;\u0026#34; try: with open(path, \u0026#34;r\u0026#34;) as fd: return yaml.safe_load(fd) except yaml.YAMLError as yml_error: print(yml_error) NETBOX_ENDPOINT = load_configuration()[\u0026#34;netbox_endpoint\u0026#34;] NETBOX_TOKEN = load_configuration()[\u0026#34;netbox_token\u0026#34;] if not NETBOX_ENDPOINT: raise OSError(\u0026#34;environmet var NETBOX_ENDPOINT not set\u0026#34;) if not NETBOX_TOKEN: raise OSError(\u0026#34;environmet var NETBOX_TOKEN not set\u0026#34;) nb = pynetbox.","title":"Generating a dynamic host inventory for ansible with Netbox"},{"content":"The World from another point of view\nFun to imagine\nKnowing versus Understanding\n","permalink":"https://memo.mx/posts/richard-feynman/","summary":"The World from another point of view\nFun to imagine\nKnowing versus Understanding","title":"Some Richard Feynman videos"},{"content":"Waju or whatever is called is a game that until this day I don\u0026rsquo;t know from where it comes. But, is fun.\nEdit: The game is based on a German game called Mensch ärgere Dich nicht\nGoal The goal of the game is very simple:\nMove your marbles for one lap from your home to your goal. Don\u0026rsquo;t worry about winning but don\u0026rsquo;t let anyone else win. Have fun. Board The board has six main components:\nDice Shallow men believe in luck or in circumstance. Strong men believe in cause and effect. \u0026ndash;Ralph Waldo Emerson\nHome This is the waiting point and one of the most infuriating places you can be in the game, it\u0026rsquo;s also the place where you start.\nStart The starting point of your home is always located in the first spot to the right of your home.\nIn the picture above is the spot that has the same colour as the home or marbles in this case.\nRoad This is where the battle happens.\nVoid I\u0026rsquo;ve seen people staying here for months!!\nThe void is a shortcut you can take only from your starting point or your corner and only if you roll a 5 or 1 respectively.\nOnce in the void, you can only move back to the road by rolling a 1 and only a 1 to your corner to the right.\nRules Play the game with two dice. Put up to 4 marbles in your home. Each turn you roll two dice, and you get two movements For each die: If you roll 1 or 6, you roll again. And repeat as many times as you roll 1 or 6. Only reroll after you finish your available movements. You can spend a movement to take to the road a marble by rolling a 1 or 6 with any of your dice. If you don\u0026rsquo;t have any marble on the road and you don\u0026rsquo;t roll a 1 or 6 your turn pass. You can choose to spend the die in only the following ways: Combine both dice and spend them in one marble. For instance, use the sum of two dice to move one marble forward. Spend one die in one marble and the second one in another marble. For instance, spend one movement to take a marble out of your home and the second one to move forward another marble. You cannot chose to spend two dice on the same marble in two movements on the same roll. If you reroll due to a 1 or 6 you can choose to move that marble again Taking out a marble from your home spends a movement and you *cannot move that marble again until your next turn or a reroll. If you don\u0026rsquo;t have any other marble to move, you can spend two separate movements in one marble. You cannot chose to spend only one die per roll. Only if you reroll one die due to 1 or 6. You can only move your marbles forward clockwise. You cannot jump or stay in the same spot as one of your marbles. If you put one of your marbles on the same spot as one of the other players, the other player\u0026rsquo;s marble must go to their respective home. Enjoy the feeling :) Bouncing back is possible, but must be your last choice Happens when the only option is to move the marble that will bounce. For instance, you should spend a movement to take a marble from home (if you have) to the road instead of bouncing. For each marble you have, move them forward on the road for one lap and place them in your goal spots. Other players cannot put their marbles on your goal spots. are you safe? You win if you put all your marbles on your goal. Schematics References https://en.m.wikipedia.org/wiki/Mensch_%C3%A4rgere_Dich_nicht ","permalink":"https://memo.mx/posts/waju/","summary":"Waju or whatever is called is a game that until this day I don\u0026rsquo;t know from where it comes. But, is fun.\nEdit: The game is based on a German game called Mensch ärgere Dich nicht\nGoal The goal of the game is very simple:\nMove your marbles for one lap from your home to your goal. Don\u0026rsquo;t worry about winning but don\u0026rsquo;t let anyone else win. Have fun. Board The board has six main components:","title":"Waju - A fun and brutal game"},{"content":" ","permalink":"https://memo.mx/posts/beyond-pep8/","summary":" ","title":"Beyond PEP 8 - Best practices for beautiful intelligible code"},{"content":"This laptop has very decent specs:\n8th Generation Intel® Core™ i7-8550U processor GPU: NVIDIA® GeForce® MX150 with 2 GB GDDR5 / Intel® UHD Graphics 620 16 GB LPDDR3 2133 MHz BT 4.1 (compatible with 3.0 and 2.1+EDR) 512 GB NVMe PCIe SSD Don\u0026rsquo;t expect running workstation level workloads in this machine but it is a wonderful dev machine.\nThings I don\u0026rsquo;t like about the laptop Palm rejection, especially this one, maybe this is Linux. Sound, it only outputs sound to two speakers under Linux and it has a werid noise under high volumes. BIOS configuration is too limited (but this is Huawei\u0026rsquo;s fault) Update 14/Jul/2019\nPalm rejection has improved a lot since I updated to the latest Touchpad software versions.\nDistro Ubuntu 18.04 with kernel 4.15.0-42-generic\nUpdate 14/Jul/2019\ndo-release-upgrade to Ubuntu 18.10 and upgraded kernel version to 5.0.0-050000-generic\nTouchpad sudo apt install acpi acpi-support acpica-tools acpid acpidump acpitail acpitool libacpi0 laptop-detect pommed xserver-xorg-input-synaptics Nvidia drivers sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update At this time, nvidia-driver-415 is the most up to date driver and the recommended one.\nsudo ubuntu-drivers autoinstall prime-select query For high-performance graphics, use:\nprime-select nvidia # log out and log in Verify nvidia is correctly installed:\nsudo lshw -C display glxinfo | grep OpenGL I\u0026rsquo;m getting readings about 12W to 17W battery discharge rate with this configuration.\nFor lower consumption, use:\nprime-select intel # log out and log in I\u0026rsquo;m getting readings about 4.5W to 6W battery discharge rate with this configuration.\nUpdate 14/Jul/2019\nAfter upgrading to kernel 5.0.0-050000-generic I\u0026rsquo;m getting discharge rates of 3.5W, not bad!!!\nDesktop Configuration i3wm sudo apt install i3wm i3lock vim ~/.config/i3/config\n# HDPI exec xrandr --dpi 220 # Applets exec --no-startup-id nm-applet exec --no-startup-id blueman-applet exec --no-startup-id gtk-redshift exec --no-startup-id megasync exec --no-startup-id dropbox start exec --no-startup-id flameshot # Lock screen bindsym $mod+l exec i3lock -c 000000 # background exec --no-startup-id /usr/bin/feh --randomize --bg-scale /path/wallpaper/* -Z And then, reload the configuration:\ni3-msg reload i3-msg restart Media keys For screen brightness and key backlights, I\u0026rsquo;m using Light\n# Sreen brightness controls bindsym XF86MonBrightnessUp exec light -A 5 bindsym XF86MonBrightnessDown exec light -U 5 # keyboard backlight controls bindsym XF86KbdBrightnessUp exec light -A 5 bindsym XF86KbdBrightnessDown exec light -A 5 For volume control, I\u0026rsquo;m using pactl\n# Volume controls bindsym XF86AudioLowerVolume exec /usr/bin/pactl set-sink-volume @DEFAULT_SINK@ \u0026#39;-5%\u0026#39; bindsym XF86AudioRaiseVolume exec /usr/bin/pactl set-sink-volume @DEFAULT_SINK@ \u0026#39;+5%\u0026#39; bindsym XF86AudioMute exec /usr/bin/pactl set-sink-mute @DEFAULT_SINK@ toggle Battery sudo apt install powertop tlp sudo powertop --calibrate sudo powertop --autotune sudo tlp start Disk I/O Following the graphical method steps on this webpage, I get the following speeds for my 512 GB NVMe PCIe SSD:\nAverage Read Rate: 1.4 GB/s (1000 samples) Average Write Read: 271.5 MB/s (1000 samples) Average Access Time: 0.11 msec (1000 samples) Maybe I\u0026rsquo;m testing it wrong, but it seems to me the write speeds are quite low.\nTroubleshooting Unsigned driver at boot If your Matebook X Pro does not boot after installing this nvidia driver or the one downloaded from nvidia\u0026rsquo;s website then disable the Secure Boot option in the BIOS.\nReconfigure the kernel sudo apt install --reinstall linux-image-generic linux-image-4.15.0-42-generic Remove old drivers sudo for FILE in $(dpkg-divert --list | grep nvidia-340 | awk \u0026#39;{print $3}\u0026#39;); do dpkg-divert --remove $FILE; done Using an eGPU in progress\nreferences https://github.com/ValveSoftware/steam-for-linux/issues/5707 https://wiki.ubuntu.com/UEFI/SecureBoot/Signing https://codeyarns.com/2013/02/07/how-to-fix-nvidia-driver-failure-on-ubuntu/ https://github.com/Syllo/nvtop https://askubuntu.com/questions/112705/how-do-i-make-powertop-changes-permanent https://int3ractive.com/2018/09/make-the-best-of-MacBook-touchpad-on-Ubuntu.html ","permalink":"https://memo.mx/posts/linux-matebook/","summary":"This laptop has very decent specs:\n8th Generation Intel® Core™ i7-8550U processor GPU: NVIDIA® GeForce® MX150 with 2 GB GDDR5 / Intel® UHD Graphics 620 16 GB LPDDR3 2133 MHz BT 4.1 (compatible with 3.0 and 2.1+EDR) 512 GB NVMe PCIe SSD Don\u0026rsquo;t expect running workstation level workloads in this machine but it is a wonderful dev machine.\nThings I don\u0026rsquo;t like about the laptop Palm rejection, especially this one, maybe this is Linux.","title":"Linux on Huawei Matebook X Pro"},{"content":"Note This is a Work-In-Progress Document and the most up-to-date information is available at: github.com/memogarcia/openstack-deployer\nDeploying OpenStack using containers allows easy customisation and flexibility on how to deploy the platform for development, testing and production environments.\nCurrent deployment: stable/queens\nHost configuration The default configuration for this environment is composed by 3 main components that need to run on the host:\nDocker Libvirtd OpenVSwitch Docker will act as the control plane for OpenStack while the host will provide the hypervisor, network and storage.\nNetwork topology This is the default network topology, 2 networks are used:\nopenstack-management-net: All openstack traffic goes through here openstack-provider-net: Instances get IPs in this network Infra services Fluentd: for logging Cadvisor: for container stats Elasticsearch: for log collection Kibana: for log visualization Portainer: for container management Third-party services Configure the third-party services needed for OpenStack to run.\nSeed MariaDB/MySQL PostgreSQL Optional Database Memcached Rabbitmq Onos Optional SDN Minio Optional Object Storage OpenStack services Keystone Glance Neutron Nova Nova-Qemu Cinder Horizon Extending OpenStack services Custom API Custom Backend Deploying OpenStack The model is a yml file describing how your environment should look like. It defines the services to run, networks, ips, volumes, dependencies, etc.\nConfigure your runtime environment by modifying model.yml.\nApply the configuration with config_processor, which will create the necessary scripts to run the environment.\nansible-playbook -i hosts/localhost config_processor.yml Config processor will create a new branch deploy where the runtime configuration will be ready for deployment.\nVerify the branch is created correctly.\ngit branch # * deploy git log # Ready for deployment Deploy OpenStack\n./scripts/docker-network-create.sh ./scripts/build.sh ./scripts/start.sh Verify installation source osrc-v3 openstack project list openstack image list openstack network list openstack server list References OpenStack installation Guide\n","permalink":"https://memo.mx/posts/openstack-containers/","summary":"Note This is a Work-In-Progress Document and the most up-to-date information is available at: github.com/memogarcia/openstack-deployer\nDeploying OpenStack using containers allows easy customisation and flexibility on how to deploy the platform for development, testing and production environments.\nCurrent deployment: stable/queens\nHost configuration The default configuration for this environment is composed by 3 main components that need to run on the host:\nDocker Libvirtd OpenVSwitch Docker will act as the control plane for OpenStack while the host will provide the hypervisor, network and storage.","title":"Deploying OpenStack with Docker"},{"content":"A Certificate Authority or CA is an entity that signs digital certificates. These digital certificates are used to validate the connection while using secure mechanisms.\nGenerating a root CA We will use a root CA to create intermediate CA\u0026rsquo;s which are trusted to sign certificates on its behalf.\nFirst, prepare the environment.\nmkdir /root/ca \u0026amp;\u0026amp; cd /root/ca mkdir certs crl newcerts private chmod 700 private touch index.txt echo 1000 \u0026gt; serial Then download the template for /root/ca/openssl.cnf from this gist and edit it.\nvim /root/ca/openssl.cnf Create the root key ca.key.pem and make sure to keep it secure.\nopenssl genrsa -aes256 -out private/ca.key.pem 4096 chmod 400 private/ca.key.pem Create a root certificate ca.cert.pem.\nopenssl req -config openssl.cnf \\ -key private/ca.key.pem \\ -new -x509 -days 10957 -sha256 -extensions v3_ca \\ -out certs/ca.cert.pem chmod 444 certs/ca.cert.pem Verify the root certificate.\nopenssl x509 -noout -text -in certs/ca.cert.pem Generating an intermediate CA It\u0026rsquo;s best practice to use intermediate CA\u0026rsquo;s instead of root CA\u0026rsquo;s to sign certificates, this practice allows a root CA to revoke a compromised intermediate CA and create a new one if necessary.\nPrepare the environment.\nmkdir /root/ca/intermediate \u0026amp;\u0026amp; cd /root/ca/intermediate mkdir certs crl csr newcerts private chmod 700 private touch index.txt echo 1000 \u0026gt; serial echo 1000 \u0026gt; /root/ca/intermediate/crlnumber Then download the template for /root/ca/intermediate/openssl.cnf from this gist and edit it.\nvim /root/ca/intermediate/openssl.cnf Create the intermediate key intermediate.key.pem.\ncd /root/ca openssl genrsa -aes256 \\ -out intermediate/private/intermediate.key.pem 4096 chmod 400 intermediate/private/intermediate.key.pem With the intermediate key create an intermediate certificate request intermediate.csr.pem for the root certificate to sign. Make sure that Common Name is different from your root CA\nopenssl req -config intermediate/openssl.cnf -new -sha256 \\ -key intermediate/private/intermediate.key.pem \\ -out intermediate/csr/intermediate.csr.pem The root CA will sign this certificate using v3_intermediate_ca extension. Make sure is valid for less time than the root CA\nopenssl ca -config openssl.cnf -extensions v3_intermediate_ca \\ -days 3650 -notext -md sha256 \\ -in intermediate/csr/intermediate.csr.pem \\ -out intermediate/certs/intermediate.cert.pem chmod 444 intermediate/certs/intermediate.cert.pem index.txt is the database file. Do NOT delete this file\nVeify the intermediate certificate.\nopenssl x509 -noout -text \\ -in intermediate/certs/intermediate.cert.pem Verify the intermediate CA against the root CA, the output should be OK.\nopenssl verify -CAfile certs/ca.cert.pem \\ intermediate/certs/intermediate.cert.pem After the verification is OK, chain the root CA and intermediate CA into a chain CA. This is only necessary if the root certificate is not installed on the client machines\ncat intermediate/certs/intermediate.cert.pem \\ certs/ca.cert.pem \u0026gt; intermediate/certs/ca-chain.cert.pem chmod 444 intermediate/certs/ca-chain.cert.pem Client certificates The intermediate certificate will be used to sign client certificates. Skip this step if you have a CSR already.\ncd /root/ca openssl genrsa -aes256 \\ -out intermediate/private/www.example.com.key.pem 2048 chmod 400 intermediate/private/www.example.com.key.pem Using 2048 bits for encryption on the client certificates is faster for TLS handshakes and lighter on the CPU but is less secure than using 4096 bits. Use it at discretion.\nUsing the private key intermediate/private/www.example.com.key.pem, create a CSR file. Skip this step if you have a CSR already.\nopenssl req -config intermediate/openssl.cnf \\ -key intermediate/private/www.example.com.key.pem \\ -new -sha256 -out intermediate/csr/www.example.com.csr.pem Signing client certificates To create a certificate, use the intermediate CA to sign the CSR.\nIf the certificate is going to use for:\nservers, use server_cert extension. authentication, use usr_cert extension. Usually, client certificates are valid for less time than the CA\u0026rsquo;s.\ncd /root/ca openssl ca -config intermediate/openssl.cnf \\ -extensions server_cert -days 375 -notext -md sha256 \\ -in intermediate/csr/www.example.com.csr.pem \\ -out intermediate/certs/www.example.com.cert.pem chmod 444 intermediate/certs/www.example.com.cert.pem Verification Verify that intermediate/index.txt contains a CN for your domain.\nVerify the certificate.\nopenssl x509 -noout -text \\ -in intermediate/certs/www.example.com.cert.pem Verify the CA certificate chain. the output should be OK.\nopenssl verify -CAfile intermediate/certs/ca-chain.cert.pem \\ intermediate/certs/www.example.com.cert.pem Distribution Distribute and/or deploy the following files:\n/root/ca/intermediate/certs/ca-chain.cert.pem /root/ca/intermediate/private/www.example.com.key.pem Only if you are signing the CSR /root/ca/intermediate/certs/www.example.com.cert.pem Next steps Sign certificates Cash in Sell out Bro down References OpenSSL Certificate Authority\n","permalink":"https://memo.mx/posts/becoming-ca/","summary":"A Certificate Authority or CA is an entity that signs digital certificates. These digital certificates are used to validate the connection while using secure mechanisms.\nGenerating a root CA We will use a root CA to create intermediate CA\u0026rsquo;s which are trusted to sign certificates on its behalf.\nFirst, prepare the environment.\nmkdir /root/ca \u0026amp;\u0026amp; cd /root/ca mkdir certs crl newcerts private chmod 700 private touch index.txt echo 1000 \u0026gt; serial Then download the template for /root/ca/openssl.","title":"Becoming a Certificate Authority (CA)"},{"content":"Note This is a Work-In-Progress Document.\nRead the docs at memogarcia/pratai-docs\nAbstract Pratai provides an incredibly flexible and resilient platform to migrate workloads to the cloud that respond to events without having to manage any server or network.\nHow it works The goal of Pratai is simple. Deploy \u0026ldquo;code\u0026rdquo; (disclaimer, from now on I will refer to code as functions), that will react to an event without worrying about anything else, the platform handles the execution. Simple right?\nIn order to achieve that, first, we need to deploy a function in a zip format for one of the languages that the platform supports, the first one is python but more will be added in the future, after this a docker image gets created with the custom function and the requirements. e.g.\n# new_module.py import numpy # yes you can install dependencies, just send a requirements.txt def local_function(payload): # you can create local functions return payload def main(payload=None): # a main function should always be declared # and using a payload as a parameter return local_function(payload) When a function gets created it will remain as inactive, waiting to be executed whenever an event happens that the function is subscribed to, could be a webhook endpoint, which can be assigned at creation time, or a message in a queue but basically, every event will spawn a container that will execute the event and then disappear.\nArchitecture Pratai is conformed of 2 major pieces, the Control Plane and the Nodes.\nControl Plane An API gateway, a database cluster and a load balancer, and agent and a scheduler runs in the control plane.\nFor the first version a API gateway built in python using flask will be made, in the future I think Golang should be a better option for it.\nAn elasticsearch cluster will power the storage of events, function metadata and cluster information.\nAnd a nginx load balancer will connect 3 instances to the API in a least_connect manner.\nPratai Nodes A Pratai node is composed by a driver and runtimes.\nWhen a new node is created it will automatically connect to the cluster and it will start polling for events.\nA driver is basically a container orchestrator like swarm, kubernetes, plain docker, etc. in this case we will use docker.\nThe runtimes are the languages supported by the platform, they are a base container image that contains an OS, a language and its dependencies, etc. that can be used by the functions the users submits. e.g.\n# seed/Dockerfile FROM ubuntu:14.04 RUN apt-get -y update RUN apt-get install -y git unzip wget # python27/Dockerfile FROM pratai/seed:latest RUN apt-get install -y python python-dev python-setuptools python-pip RUN pip install pip --upgrade # python27_template.txt FROM pratai/python27:latest RUN wget {zip_location} RUN unzip {zip_file} RUN pip install -r requirements.txt RUN mkdir /etc/pratai/ RUN mkdir /var/log/pratai/ RUN cp new_module.py /etc/pratai/ RUN git clone \u0026#34;repo_with_runtimes\u0026#34; CMD [\u0026#34;python\u0026#34;, \u0026#34;/pratai-runtimes/runtimes/python27/server.py\u0026#34;] Distributed Queues ZeroMQ is the choice for queuing and passing messages in pratai using the PUSH/PULL architecture we can create a pipelines of messages that can be distributed across multiple nodes.\nWe will have a producer and a collector running in the scheduler, and consumers running in the Pratai nodes, one consumer should be spawned per thread.\nEvents A function can react to any event coming through webhooks or messages in a queue, even events that happen in a database can trigger a function, is important to notice that a response of a function is an event, so it can trigger so chaining functions to build pipelines of data processing is easy with Pratai.\nThere are 2 kinds of events, async and wait_for_response\nAsync This is the default event for pratai, it will take a request or a message and process it asynchronously, then, you can collect the logs or responses, by default the response gets stored in a collector queue, that can send this response as an input for other functions.\nYou can use the async event in the following cases:\n1: Async + Webhook\nThis is the default behaviour, in which a function will be executed asynchronously when an HTTP POST requests hits its endpoint.\npratai function-create {name} --type async --event webhook 2: Async + Message\nA function created with this configuration will executed asynchronously when a message arrives in the event queues available for the platform.\npratai function-create {name} --type async --event message --subscribe_to {event_id} 3: Async + Timer\nA function created with this configuration will executed asynchronously every time a timer sends an event, the frequency of the events are set in minutes.\npratai function-create {name} --type async --event timer --frequency 5 Wait For Response This is a request that works like a typical web server, you send a request and you wait for a response and only works for webhooks events\npratai function-create {name} --type wait_for_response --event webhook Components API Gateway The API is the main interface for incoming webhook requests and for platform configuration.\nAgent The Agent is the main interface for events in queues and cron jobs.\nclient python-prataiclient is the component that allows the user to interact with the api from the command line interface, with it you can do stuff like this:\npratai function-create music_tag --file /path/to/zip --description \\ \u0026#34;extract metadata from music files\u0026#34; --memory 128 pratai function-list Because this is OpenStack you should pass credentials to interact with the platform\nexport OS_USERNAME=user export OS_PASSWORD=password export OS_TENANT_NAME=pratai_tenant export OS_PRATAI_URL=http://192.168.33.9:9096 export OS_IDENTITY_API_VERSION=3 export OS_AUTH_URL=http://192.168.33.9:5000/v3 export OS_PROJECT_NAME=pratai_tenant export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default Drivers A driver is a backend that orchestrate a container that contains the custom code.\nRuntimes A runtime is a language that is supported by the platform, it contains the language and its dependencies.\nScheduler The scheduler primarily consists of a set of Python daemons, though it requires and integrates with a number of native system components for databases and messaging capabilities.\n1; Scheduler\nThe API and the Agent push messages to this queue which will be pre-processed before being distributed among the pratai nodes.\n2; Collector\nWhen a function finish the function execution it will send the result and status here in order to be stored in the database afterwards.\nSecurity \u0026amp; Secrets The functions that interact with external services most often that not they require to use credentials to connect, for this, Barbican has been proposed to help with this scenario.\nWe definitely recommend using tokens instead of user/passwords when possible.\nCommunity Join us at #pratai irc channel in freenode\nRepositories memogarcia/pratai-docs memogarcia/pratai-agent memogarcia/pratai-api memogarcia/pratai-scheduler memogarcia/pratai-runtimes memogarcia/pratai-drivers References The Reactive Manifesto Cloud Design Patterns ","permalink":"https://memo.mx/posts/pratai/","summary":"Note This is a Work-In-Progress Document.\nRead the docs at memogarcia/pratai-docs\nAbstract Pratai provides an incredibly flexible and resilient platform to migrate workloads to the cloud that respond to events without having to manage any server or network.\nHow it works The goal of Pratai is simple. Deploy \u0026ldquo;code\u0026rdquo; (disclaimer, from now on I will refer to code as functions), that will react to an event without worrying about anything else, the platform handles the execution.","title":"Pratai, event driven platform for OpenStack"},{"content":"My blog is intended as a self reference and I don’t provide any support unless specified.\nIf you find anything useful, please どうぞ! License\nDO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE Version 2, December 2004 Copyright (C) Guillermo Ramirez Garcia \u0026lt;root@memo.mx\u0026gt; Everyone is permitted to copy and distribute verbatim or modified copies of this license document, and changing it is allowed as long as the name is changed. DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION 0. You just DO WHAT THE FUCK YOU WANT TO. ","permalink":"https://memo.mx/about/","summary":"My blog is intended as a self reference and I don’t provide any support unless specified.\nIf you find anything useful, please どうぞ! License\nDO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE Version 2, December 2004 Copyright (C) Guillermo Ramirez Garcia \u0026lt;root@memo.mx\u0026gt; Everyone is permitted to copy and distribute verbatim or modified copies of this license document, and changing it is allowed as long as the name is changed. DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION 0.","title":"About"},{"content":" In this series of posts I propose a not so perfect analogy for computer networks represented as buildings.\nNetworks and subnets or building layouts Switches and routers or how rooms can communicate between floors ","permalink":"https://memo.mx/understanding-networks-by-analogy/","summary":" In this series of posts I propose a not so perfect analogy for computer networks represented as buildings.\nNetworks and subnets or building layouts Switches and routers or how rooms can communicate between floors ","title":"Understanding computer networks by analogy"}]