[{"content":"I’m trying to learn everything at once,\nI’m trying to do everything at once\n\u0026hellip;\nExcept what I should be doing right now.\n","permalink":"https://memooo.ooo/posts/busy/","summary":"I’m trying to learn everything at once,\nI’m trying to do everything at once\n\u0026hellip;\nExcept what I should be doing right now.","title":"Why am I so busy"},{"content":"Kubernetes is becoming a monster and as it grows it becomes more and more challenging for newcomers to understand it.\nMy goal is to demystify its components.\nMain components There are two main components in a Kubernetes cluster.\nMaster nodes Worker nodes The only difference is the workloads they run. You can assign metadata to these nodes to schedule specific workloads on each node or type of node. For Users Containers A container is a filesystem and process wrapped in a “box” with some labels on it. Volumes A volume is a filesystem that lives outside the container. Pods Organize containers (one or more) and a volume(s) as a single unit This is the basic unit on which Kubernetes works. A pod has an IP (or more depending on your CNI) ConfigMaps Are just files that are mounted in pods (specifically, they are mounted in a container) Secrets Are just files that are mounted in the pods but the data is encoded. (specifically, they are mounted in a container). Note This is insecure, take a look at different approaches if security is important\nDeployment Is a way to tell Kubernetes the desired state of your pods, services, and volumes. It also tells Kubernetes how you want to deploy your pods, how many replicas (replicaset) or if you want a fixed amount of pods for a database cluster for example.(statefulset) Service Service has an IP that redirects traffic to your pod(s) Selectors This is how you map a service to a pod using labels Ingress Create rules to allow traffic from outside the cluster to inside the cluster Imperative vs Declarative mode You tell Kubernetes what you want to do: kubectl create ns namespace\nYou tell Kubernetes the desired state of your resource. kubectl create -f resource.yml\nFor Operators Kubernetes control plane is composed of the following components:\nkube-apiserver Runs in master nodes Creates resources in the cluster Custom resource definitions is a way to add features to the api Stores things in etcd or sqlite (for k3s) kube-scheduler Runs in the master node (a single instance at a time) It watches the kube-apiserver and schedules resources in nodes You can specify or give hints to the scheduler where you want your resources to live Custom schedulers are allowed, and more than 1 scheduler is allowed kube-controller-manager Runs in the master node (a single instance at a time) Delegates tasks to the rest of the managers to manage resources namespace-manager deployment-manager replicase-manager secret-manager operators are custom controller-managers In other words, *-managers, are a bunch of for loops watching for changes in the Kubernetes API and applying them to the cluster. They also watch for changes in the cluster against the desired state and make sure they match it. kubelet Runs in all nodes It creates containers It talks to container runtimes (rkt, docker, podman, etc.) Interacts with liveness probes Interacts with readiness probes kube-proxy Runs in all nodes It creates services and its underlying iptables anything can be extended or replaced\nKubernetes networking Pod Has a unique IP and has a CIDR Services A service has a port that listens to traffic from outside a pod(s) and sends it to the pod(s) Is an abstraction to give a name instead of an IP for other services to reach a pod(s) Types: ClusterIP The ClusterIP does not “live” in your interfaces, is an iptables rule created by kube-proxy manages this NodePort Still has a ClusterIP but also has a port in the node where the pod lives It is also an iptables rule LoadBalancer This is cloud-specific on how it works Point your client to your load balancer ip:port instead of the node_ip:port ","permalink":"https://memooo.ooo/posts/understanding-kubernetes/","summary":"Kubernetes is becoming a monster and as it grows it becomes more and more challenging for newcomers to understand it.\nMy goal is to demystify its components.\nMain components There are two main components in a Kubernetes cluster.\nMaster nodes Worker nodes The only difference is the workloads they run. You can assign metadata to these nodes to schedule specific workloads on each node or type of node. For Users Containers A container is a filesystem and process wrapped in a “box” with some labels on it.","title":"Demystifying Kubernetes"},{"content":"Looming recession !! , third world war !!, climate change !!\nMeanwhile taquito…\n","permalink":"https://memooo.ooo/posts/taquito/","summary":"Looming recession !! , third world war !!, climate change !!\nMeanwhile taquito…","title":"A lesson from my dog"},{"content":"The rapid development and adoption of cloud-native stacks that brings a better developer experience, security, reproducibility and speed at which organizations deliver value are leaving more traditional stacks behind. Hence, there is more pressure from the markets, organizations and developers to bring those stacks into a more modern era.\nWe often wonder whether the same techniques and toolchains of these modern stacks can be used to configure not-so-modern applications or infrastructures.\nThe answer is yes.\nBut how can help our organizations to adopt these modern techniques and toolchains?\nFor example, the competitive landscape of tools and services that surrounds the services that provide value has made them simple to integrate with any modern stack, more often than not with a simple click. These tools and services provide out-of-the-box functionality that can be leveraged not only for customers but your operations teams. (i.e. monitoring, tracing, backup, etc).\nThe goal of these tools is to become easy to deploy, easy to configure, pluggable, and be as self-described as possible. They do so by leveraging source control systems and CI/CD pipelines to reduce the complexity between a change being made and production deployment.\nOur goal is to leverage the same procedure.\nHowever, what happens to applications and infrastructure that weren’t designed in such a way? can they still leverage a modern approach? Yes. Using GitOps.\nGitOps is a way to implement continuous deployments toolchains for your applications/infrastructure, and it focuses on 3 main areas:\nDeveloper centric experience Infrastructure as code, store the state of your applications/infrastructure as configuration files Reconciliation loops that make the desired state of your infrastructure as code (IaaC) match what’s deployed. And that’s it! At its core, a modern GitOps toolchain consists of these 3 steps which can be easily translated to any environment.\nNow is easy to picture how can an application/infrastructure be modernized with GitOps.\nDeveloper centric experience Developers can leverage their tooling and workflows to keep pushing changes to git repositories, using all or any git workflows they are familiar with.\nInfrastructure as code Now the state of your application/infrastructure is defined as configuration and stored in git some capabilities are starting to become more accessible:\nEasy and fast error recovery, you can always redeploy to a known working state, Self-documented deployments, your configuration is your deployment, unknown states coming from manual changes are minimized, Redeployment in new environments is easier and can be automated, Immutable application/infrastructure deployments, Each commit is a deployment, each commit is value delivered. Reconciliation tools Reconciliation tools take the desired state from git and make sure it matches your environment. These tools work mainly in two paradigms:\nDeclarative infrastructure as code Imperative infrastructure as code Choosing declarative or imperative definitions for your infrastructure as code is more often than not, dictated by your organization and/or team and thus it can help you to narrow the options to choose reconciliation tools.\nFor example:\nDeclarative IaaC can use Argo CD to deploy and keep your environment synchronized in a single step using a reconciliatory loop. In this example, a tool like Argo CD will ensure that your environment is always in the desired state by observing both the desired state from your IaaC and what’s currently deployed. Imperative IaaC can use Jenkins to execute an Ansible playbook every time a commit is pushed to your git repository or periodically. Even though this is a more traditional approach, it can simulate a reconciliatory loop and give you the same result in your environment. Why GitOps is important and what value does it bring to my organization? Automation is a competency that any organization must master to bring order in this chaotic landscape. Once people, code, and tools are in place, new automation opportunities to modernize start to become more apparent to the organization. GitOps is just one of the ways to ensure control and confidence over how, when, and what you deliver.\nFAQ.\nIs my organization ready for GitOps? In short, most probably yes. Do I need specific tooling to modernize our current infrastructure? No, Using GitOps doesn’t mean using a specific set of tools, is a framework for automation best practices. ","permalink":"https://memooo.ooo/posts/gitops-in-enterprise/","summary":"The rapid development and adoption of cloud-native stacks that brings a better developer experience, security, reproducibility and speed at which organizations deliver value are leaving more traditional stacks behind. Hence, there is more pressure from the markets, organizations and developers to bring those stacks into a more modern era.\nWe often wonder whether the same techniques and toolchains of these modern stacks can be used to configure not-so-modern applications or infrastructures.","title":"How to use GitOps in a non-cloud-native environment"},{"content":"Be very carrefull in your setup : any misconfiguration make all the git config to fail silently !\nSetup multiple git ssh identities for git Generate your SSH keys as per your git provider documentation. Add each public SSH keys to your git providers acounts. In your ~/.ssh/config, set each ssh key for each repository as in this exemple: Host github.com HostName github.com User git IdentityFile ~/.ssh/github_private_key IdentitiesOnly=yes Host gitlab.com Hostname gitlab.com User git IdentityFile ~/.ssh/gitlab_private_key IdentitiesOnly=yes Setup dynamic git user email \u0026amp; name depending on folder Require git 2.13+ for conditional include support.\nThe idea here is to use a different git user name and email depending on the folder you are in.\nIn your ~/.gitconfig, remove the [user] block and add the following (adapt this exemple to your needs) : [includeIf \u0026#34;gitdir:~/src/personal/\u0026#34;] path = .gitconfig-personal [includeIf \u0026#34;gitdir:~/src/work/\u0026#34;] path = .gitconfig-work In your ~/.gitconfig-personal, add your personnal user informations: [user] email = user.personal@email.com name = personal_username In your ~/.gitconfig-work, add your professional user informations: [user] email = user.professional@company.com name = professional_username Setup a GPG key If you need to add a GPG key and bind it to a user to sign your commits, you can do so like this:\nYou should have GPG installed and configured like the GPG suite\nAdd the GPG key ID to your ~/.gitconfig-{PROFILE} config and enable commit signing: [user] email = your.mail@domain.com name = Your NAME signingkey = SIGNING_KEY_ID [commit] gpgsign = true Make sure to register the right GPG binary in your ~/.gitconfig: [program] pgp = /path/to/your/gpg2/bin Test your setup Now each repository will use the custom user info setup depending on the top-level folder. Check your settings are taken into account, for instance in ~/src/git/personal/ : cd ~/src/git/personal/ git config --get user.email git config --get user.name git config --get user.signingkey Do the same for each folder you have setup. You can also display and check the global git config: git config --list --global Or just the local config for the repository folder you are in: git config --list Exporting Public GPG key gpg --list-keys gpg --armor --export {KEY-ID} References Generating a new GPG key Telling Git about your signing key ","permalink":"https://memooo.ooo/posts/git-multiple-identities/","summary":"Be very carrefull in your setup : any misconfiguration make all the git config to fail silently !\nSetup multiple git ssh identities for git Generate your SSH keys as per your git provider documentation. Add each public SSH keys to your git providers acounts. In your ~/.ssh/config, set each ssh key for each repository as in this exemple: Host github.com HostName github.com User git IdentityFile ~/.ssh/github_private_key IdentitiesOnly=yes Host gitlab.com Hostname gitlab.","title":"Setup multiple git identities and pgp keys"},{"content":"WSL configuration Install i3\nsudo apt install i3 -y Create an init script\nvim ~/src/scripts/i3launch.sh #!/bin/zsh source ~/.zshrc # If not running interactively, don\u0026#39;t do anything [ -z \u0026#34;$PS1\u0026#34; ] \u0026amp;\u0026amp; return export DISPLAY=$(awk \u0026#39;/nameserver / {print $2; exit}\u0026#39; /etc/resolv.conf 2\u0026gt;/dev/null):0 export LIBGL_ALWAYS_INDIRECT=1 dbus_status=$(service dbus status) if [[ $dbus_status = *\u0026#34;is not running\u0026#34;* ]]; then sudo service dbus --full-restart fi i3 To run WSL2 as root\nwsl.exe -d Ubuntu-20.04 -u root -- /bin/bash Windows configuration Install vcxsrv from powershell\nwinget install vcxsrv With WSL2 you need to configure your firewall to allow WSL and vcxsrv to communicate.\nSearch for Windows Defender Firewall with Advanced Security and do the following:\nCreate an inbound rule:\nname: wsl2 rule type: port port type: tcp port number: 6000 Narrow the scope of your inbound rule:\nRight click -\u0026gt; Properties -\u0026gt; scope -\u0026gt; Remote IP addresses -\u0026gt; Add 172.16.0.0/12 Search for VcXsrv windows xserver inbound rules and make sure the 4 rules are enabled and in allow mode\nStartup i3 script from Windows vcxsrv.vbs\ncode vcxsrv.vbs Set shell = CreateObject(\u0026#34;WScript.Shell\u0026#34; ) shell.Run \u0026#34;\u0026#34;\u0026#34;C:\\Program Files\\VcXsrv\\vcxsrv.exe\u0026#34;\u0026#34; :0 -screen 0 @1 -ac -engine 1 -nodecoration -wgl\u0026#34; WScript.Sleep 200 shell.Run \u0026#34;wsl.exe -d YOUR_DISTRO -u YOUR_USER -- /bin/zsh ~/src/scripts/i3launch.sh\u0026#34;, 0 And just run your script from powershell\n.\\vcxsrv.vbs You should see this screen.\ni3 configuration Up to you.\nReferences Running i3 Desktop with WSL on Windows 10 How to set up working X11 forwarding on WSL2 WSL Windows Toolbar Launcher ","permalink":"https://memooo.ooo/posts/i3wm-wsl2/","summary":"WSL configuration Install i3\nsudo apt install i3 -y Create an init script\nvim ~/src/scripts/i3launch.sh #!/bin/zsh source ~/.zshrc # If not running interactively, don\u0026#39;t do anything [ -z \u0026#34;$PS1\u0026#34; ] \u0026amp;\u0026amp; return export DISPLAY=$(awk \u0026#39;/nameserver / {print $2; exit}\u0026#39; /etc/resolv.conf 2\u0026gt;/dev/null):0 export LIBGL_ALWAYS_INDIRECT=1 dbus_status=$(service dbus status) if [[ $dbus_status = *\u0026#34;is not running\u0026#34;* ]]; then sudo service dbus --full-restart fi i3 To run WSL2 as root\nwsl.exe -d Ubuntu-20.04 -u root -- /bin/bash Windows configuration Install vcxsrv from powershell","title":"i3 running on WSL2"},{"content":"Switches or \u0026ldquo;The floor concierge\u0026rdquo; Imagine that you want to send a package from room 69 to room 62. In a typical building you cannot go to your neighbor and give them the package, it’s rude, you need to do it through the floor concierge.\nThe concierge or switch has a table of everyone\u0026rsquo;s door numbers:\n| Floor | Room number | Door number | |-------|-------------|-------------| | 6 | 602 | 1 | | 6 | 609 | 1 | Remember that each room can have many doors.\nIn reality the switch has a table that looks like this:\n| Vlan | MAC Address | Port | |------|-------------------|-------| | 6 | aa:aa:aa:aa:aa:aa | fa0/2 | | 6 | ff:ff:ff:ff:ff:ff | fa0/3 | The concierge knows to which door exactly to deliver the package.\nIn other words, a switch connects computers in a network.\nNow, the concierge\u0026rsquo;s capacity is limited by several factors:\nSize of the sending door - speed of your interface Size of the receiving door - speed of their interface Size of the hallway in which the package is moving - \u0026ldquo;medium speed\u0026rdquo; (cable, wireless, etc.) Size of the concierge desk - switch port speed Your concierge (switch) have multple roles as well.\nPackage monitoring - Traffic monitoring Package priority - QoS Bundle many doors for improved speed or redundancy - link aggregation Block packages from unwanted rooms - MAC filtering or port disabling Door monitoring - SNMP And many more But, what if you want to send your package to your neighbor in the 2nd floor?\nEnter the\u0026hellip;\nRouters or \u0026ldquo;The building concierge\u0026rdquo; This building concierge or router is the one handling the packages from one floor to another.\nIn other words, a router connects many networks together.\nSame as the floor concierge, it has a table of rooms, but it uses the room number rather than the door number.\n| Floor destination | Room quantity | Elevator door number | Room number | Notes | |:-----------------:|:-------------:|:--------------------:|:-----------:|------------------| | 6 | 256 | 1 | 9 | Easy to deliver | | 2 | 256 | 1 | 2 | Deliver at night | This table will translate to something like this:\n| Network destination | Netmask | Gateway | Interface | Metric | |:-------------------:|:---------------:|:-----------:|:-----------:|--------| | 192.168.6.0 | 255.255.255.255 | 192.168.6.1 | 192.168.6.9 | 1 | | 192.168.2.2 | 255.255.255.255 | 192.168.2.1 | 192.168.2.1 | 10 | The building concierge (router) has an entry for notes (metrics) to decide which route or time is best to deliver the package to its destination.\nBut it cannot do it alone, it needs the floors to be connected somehow\u0026hellip; like with an elevator or gateway\nThis elevator behaves like a normal room, in the sense that it has a door, but it can move from floor to floor, which means that it has a door \u0026ldquo;assigned\u0026rdquo; in floor 2 and 6.\nThis door is managed by the building concierge (router) not the floor concierge (switch). When a package needs to leave the floor, the floor concierge sends the package to the building concierge and it deliver the package to its destination.\nFrom the floor concierge (switch) perspective, is another room with a door number (MAC Address) and it can send/receive packages in the same way as if the packages were in the same floor.\nOver time, the building concierge (router) can learn new ways to deliver the packages more efficiently and work with packages based on priority or QoS.\nSeries Networks and subnets or building layouts Switches and Routers or how rooms can communicate between floors Network protocols or how rooms communicate Internet, public IPs, NAT, DNS or how to connect rooms between buildings High performance networking or how to speed up the communication between rooms using high speed roads and other techniques SDN or dynamic floor arrangement Contributions If you want contribute, please send a pull request or open an issue to this repo\n","permalink":"https://memooo.ooo/posts/understanding-computer-networks-by-analogy-part-2/","summary":"Switches or \u0026ldquo;The floor concierge\u0026rdquo; Imagine that you want to send a package from room 69 to room 62. In a typical building you cannot go to your neighbor and give them the package, it’s rude, you need to do it through the floor concierge.\nThe concierge or switch has a table of everyone\u0026rsquo;s door numbers:\n| Floor | Room number | Door number | |-------|-------------|-------------| | 6 | 602 | 1 | | 6 | 609 | 1 | Remember that each room can have many doors.","title":"Understanding Computer Networks by Analogy - Part 2 - Switches and Routers"},{"content":"Prologue There are many ways to learn something new and no approach is best. But, what works best for me is a combination of trial and error and learning by first principles. But, as someone once said:\nKnowledge is only valuable when it leads to taking action on an idea.\nAnd as long as you share that knowledge you are taking action on an idea.\nJust one big question is: how to share them? I find that analogies are a great tool to do that. They let you group together many ideas and present them as a whole to new audiences in a very fun way.\nIn this series of posts I propose a not so perfect analogy for computer networks represented as buildings.\nNetworks or \u0026ldquo;The buildings\u0026rdquo; This building represents a \u0026ldquo;network\u0026rdquo; of \u0026ldquo;connected rooms\u0026rdquo;.\nThink of each room as a computer with a room number as an IP address.\nroom + room number == computer + ip address In this network, each room can communicate directly to any other room using network protocols that will be represented as a packages.\nBut now you are wondering\u0026hellip; I don\u0026rsquo;t want to send packages to all my neighbors\u0026hellip;I just want to communicate with specific ones.\nEnter the\u0026hellip;\nSubnets or \u0026ldquo;The floors\u0026rdquo; By dividing your building into floors you can now isolate the communication between them. Neighbors in the 6th floor cannot communicate with neighbors in the 9th floor.\nThis is called a subnet\nYou can identify the rooms by the room number or IP address.\nExample room 69 can be idenfied as:\nfloor 6, room 9 or as an IP address:\n192.168.6.9 This room number is unique to your building but the next building can have a room 69 in its building as well (I will cover this in a later post about private vs public IPs).\nComputers or \u0026ldquo;The rooms\u0026rdquo; In reality, your room number is not tied to the physical space in the building but to the door that allows access to it.\nWhich means that your room can have many doors with different properties:\nA door for maintenance, A door for guests, And so on. Within the floor (subnet) your door have a unique number called MAC address\nIn other words, your room is identified by an door number rather than a room number within a subnet.\nroom -\u0026gt; door 1 -\u0026gt; room 69 or as a relation between Computer, MAC and IP\ncomputer interface eth0 -\u0026gt; ff:ff:ff:ff:ff:ff -\u0026gt; 192.168.6.9 Series Networks and subnets or building layouts Switches and Routers or how rooms can communicate between floors Network protocols or how rooms communicate Internet, public IPs, NAT, DNS or how to connect rooms between buildings High performance networking or how to speed up the communication between rooms using high speed roads and other techniques SDN or dynamic floor arrangement Contributions If you want contribute, please send a pull request or open an issue to this repo\n","permalink":"https://memooo.ooo/posts/understanding-computer-networks-by-analogy/","summary":"Prologue There are many ways to learn something new and no approach is best. But, what works best for me is a combination of trial and error and learning by first principles. But, as someone once said:\nKnowledge is only valuable when it leads to taking action on an idea.\nAnd as long as you share that knowledge you are taking action on an idea.\nJust one big question is: how to share them?","title":"Understanding Computer Networks by Analogy - Part 1 - Networks and subnets"},{"content":"Fuzzy Search documentation from the CLI.\nSee it in action here https://terminalizer.com/view/2c3935cf1418\nDisclaimer This tool was built to learn FZF capabilities. Feel free to use it or extend it.\nUsage doc-fzf ansible doc-fzf ansible -q yum Installation pip3 install doc-fzf Verify your installation:\ndoc-fzf -h usage: doc-fzf.py [-h] [-q QUERY] module_name doc-fzf. positional arguments: module_name Name of the module to search optional arguments: -h, --help show this help message and exit -q QUERY Query the docs Extending Doc-FZF doc-fzf is a modular application. It can load modules at runtime that scrap websites in any way you like.\nAny module should always contain:\nclass name must always be Screapper(FZFDoc) self.documentation_url attribute def get_documentation(self): function that must always return a tuple (\u0026ldquo;url\u0026rdquo;, \u0026ldquo;description\u0026rdquo;) from doc_fzf.pyfzf import FZFDoc class Scrapper(FZFDoc): def __init__(self): self.base_url = \u0026#34;https://docs.python.org/3\u0026#34; self.documentation_url = \u0026#34;{0}/py-modindex.html\u0026#34;.format(self.base_url) FZFDoc.__init__(self, self.documentation_url) def get_documentation(self): \u0026#34;\u0026#34;\u0026#34; Return a tuple of (url, description) \u0026#34;\u0026#34;\u0026#34; docs = get_online_documentation() for doc in docs: yield (doc.url, doc.description) Here is the ansible documentation example\nRoad Map Module definition FZFDoc base class File system cache layer Load dynamic modules References Doc-FZF: Modular CLI Documentation Fuzzy Finder fzf, A command-line fuzzy finder iterfzf, Pythonic interface to fzf, a CLI fuzzy finder ","permalink":"https://memooo.ooo/posts/doc-fzf/","summary":"Fuzzy Search documentation from the CLI.\nSee it in action here https://terminalizer.com/view/2c3935cf1418\nDisclaimer This tool was built to learn FZF capabilities. Feel free to use it or extend it.\nUsage doc-fzf ansible doc-fzf ansible -q yum Installation pip3 install doc-fzf Verify your installation:\ndoc-fzf -h usage: doc-fzf.py [-h] [-q QUERY] module_name doc-fzf. positional arguments: module_name Name of the module to search optional arguments: -h, --help show this help message and exit -q QUERY Query the docs Extending Doc-FZF doc-fzf is a modular application.","title":"Modular CLI Documentation Fuzzy Finder"},{"content":"Usage:\npip install pynetbox ansible ansible all -i hosts/env -m setup --tree /tmp/facts/env #!/opt/netbox/bin/python import argparse import json import os import sys import pynetbox import yaml import urllib3 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) if sys.version_info \u0026lt; (3, 6): print(\u0026#34;Python 3.6 is required\u0026#34;) sys.exit(2) def to_json(in_dict): return json.dumps(in_dict, sort_keys=True, indent=4) def load_configuration(path=\u0026#34;/etc/ansible/netbox.yml\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Load netbox configuration /etc/ansible/netbox.yml \u0026#34;\u0026#34;\u0026#34; try: with open(path, \u0026#34;r\u0026#34;) as fd: return yaml.safe_load(fd) except yaml.YAMLError as yml_error: print(yml_error) NETBOX_ENDPOINT = load_configuration()[\u0026#34;netbox_endpoint\u0026#34;] NETBOX_TOKEN = load_configuration()[\u0026#34;netbox_token\u0026#34;] if not NETBOX_ENDPOINT: raise OSError(\u0026#34;environmet var NETBOX_ENDPOINT not set\u0026#34;) if not NETBOX_TOKEN: raise OSError(\u0026#34;environmet var NETBOX_TOKEN not set\u0026#34;) nb = pynetbox.api(NETBOX_ENDPOINT, NETBOX_TOKEN, ssl_verify=False) def parse_args(): parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--list\u0026#39;, action=\u0026#39;store_true\u0026#39;) parser.add_argument(\u0026#39;--host\u0026#39;, action=\u0026#39;store\u0026#39;) parser.add_argument(\u0026#39;--introspection\u0026#39;, action=\u0026#39;store_true\u0026#39;) return parser.parse_args() def get_site(site_name): site = nb.dcim.sites.get(name=site_name) return site def get_vms(site_name): platform = nb.dcim.platforms.get(name=site_name) vms = nb.virtualization.virtual_machines.filter(platform_id=platform.id) return vms def get_devices(site_name): platform = nb.dcim.platforms.get(name=site_name) devices = nb.dcim.devices.filter(platform_id=platform.id) return devices def get_roles(): roles = nb.dcim.device_roles.all() return roles def get_role(server): try: try: return server.role except: return server.device_role except Exception as error: return \u0026#34;ungrouped\u0026#34; def which_tenant(server): try: if server.tenant: return str(server.tenant) else: # print(\u0026#34;No tenant\u0026#34;) return None except Exception as error: print(error) def which_gmn(server): try: if \u0026#34;management\u0026#34; in server.tags: return \u0026#34;management\u0026#34; else: return \u0026#34;no_management\u0026#34; except Exception as error: print(error) def get_tenants(): tenants = nb.tenancy.tenants.all() return tenants def get_device_role(site, role_name): platform = nb.dcim.platforms.get(name=site) role = nb.dcim.device_roles.get(platform_id=platform.id, name=role_name) if not role: print(\u0026#34;Role not found\u0026#34;) sys.exit(2) return role.id def get_vip(site, servers): vms = get_vms(site) proxy_vms = [] for vm in vms: if str(vm.role).lower() == \u0026#34;proxy-servers\u0026#34;: proxy_vms.append(vm) for vm in proxy_vms: ips = nb.ipam.ip_addresses.filter(virtual_machine_id=vm.id) for ip in ips: if str(ip.role) == \u0026#34;VIP\u0026#34;: return ip.address else: return \u0026#34;\u0026#34; def get_tenant_role_map(path=\u0026#34;/etc/ansible/role-tenant-map.json\u0026#34;): \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; try: with open(path, \u0026#34;r\u0026#34;) as fd: return json.load(fd) except Exception as error: print(error) def generate_inventory(site, servers): \u0026#34;\u0026#34;\u0026#34; Generate ansible groups based on roles and tenants \u0026#34;\u0026#34;\u0026#34; role_tenant_map = get_tenant_role_map() roles = get_roles() tenants = get_tenants() # base json|yaml structure inventory = { \u0026#34;all\u0026#34;: {\u0026#34;hosts\u0026#34;: []}, \u0026#34;management\u0026#34;: {\u0026#34;children\u0026#34;: {}}, \u0026#34;no_management\u0026#34;: {\u0026#34;children\u0026#34;: {}, \u0026#34;vars\u0026#34;: {}}, \u0026#34;ungrouped\u0026#34;: {\u0026#34;children\u0026#34;: {}}, \u0026#34;_meta\u0026#34;: {\u0026#34;hostvars\u0026#34;: {}} } proxy_vip = get_vip(site, servers).split(\u0026#34;/\u0026#34;)[0] if proxy_vip: inventory[\u0026#34;no_gmn\u0026#34;][\u0026#34;vars\u0026#34;] = { \u0026#34;ansible_ssh_common_args\u0026#34;: f\u0026#34;\u0026#39;-o ProxyJump={proxy_vip} -o StrictHostKeyChecking=no\u0026#39;\u0026#34; } for tenant in tenants: inventory[tenant.name] = {\u0026#34;children\u0026#34;: {}} for role in role_tenant_map[tenant.name]: inventory[tenant.name][\u0026#34;children\u0026#34;][role] = {} for role in roles: inventory[role.name] = {\u0026#34;hosts\u0026#34;: []} if role.name == \u0026#34;proxy-servers\u0026#34;: inventory[\u0026#34;management\u0026#34;][\u0026#34;children\u0026#34;][role.name] = {} else: inventory[\u0026#34;no_management\u0026#34;][\u0026#34;children\u0026#34;][role.name] = {} for server in servers: ip = str(server.primary_ip).split(\u0026#34;/\u0026#34;)[0] role = str(get_role(server)) inventory[\u0026#34;all\u0026#34;][\u0026#34;hosts\u0026#34;].append(server.name) inventory[\u0026#34;_meta\u0026#34;][\u0026#34;hostvars\u0026#34;][server.name] = { \u0026#34;ansible_host\u0026#34;: f\u0026#34;{ip}\u0026#34; } if role != \u0026#34;None\u0026#34;: inventory[role][\u0026#34;hosts\u0026#34;].append(server.name) return to_json(inventory) def get_introspection_data(site_name): \u0026#34;\u0026#34;\u0026#34; site_name: str: SITE1, SITE2, etc. return list(dicts) [ { \u0026#34;mac\u0026#34;: \u0026#34;pxe mac\u0026#34;, \u0026#34;arch\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;pm_type\u0026#34;: \u0026#34;pxe_ilo\u0026#34;, \u0026#34;pm_user\u0026#34;: \u0026#34;static per site\u0026#34;, \u0026#34;pm_password\u0026#34;: \u0026#34;static per site\u0026#34;, \u0026#34;pm_address\u0026#34;: \u0026#34;IPMI address\u0026#34; \u0026#34;name\u0026#34;: \u0026#34;position name\u0026#34; }, ] steps: query devices api on a given site (platform) for computes and controllers get pm_user and pm_password from somewhere get IPMI address \u0026#34;\u0026#34;\u0026#34; openstack_nodes = [] platform = nb.dcim.platforms.get(name=site_name) devices = nb.dcim.devices.filter(platform_id=platform.id) for device in devices: if device.custom_fields[\u0026#34;openstack_device\u0026#34;]: d = { \u0026#34;name\u0026#34;: device.name, \u0026#34;mac\u0026#34;: [device.custom_fields[\u0026#34;openstack_pxeboot_mac\u0026#34;]], \u0026#34;pm_type\u0026#34;: \u0026#34;pxe_ilo\u0026#34;, \u0026#34;arch\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;pm_user\u0026#34;: os.environ.get(\u0026#34;OS_INTROSPECTION_USER\u0026#34;, None), \u0026#34;pm_password\u0026#34;: os.environ.get(\u0026#34;OS_INTROSPECTION_PASSWORD\u0026#34;, None) } interfaces = nb.dcim.interfaces.filter(device_id=device.id) for i in interfaces: if i.name == \u0026#34;ILO\u0026#34;: ilo_ip = nb.ipam.ip_addresses.filter(interface_id=i.id)[0] d[\u0026#34;pm_address\u0026#34;] = ilo_ip.address.split(\u0026#34;/\u0026#34;)[0] openstack_nodes.append(d) return to_json(openstack_nodes) if __name__ == \u0026#34;__main__\u0026#34;: args = parse_args() # Are \u0026#34;-\u0026#34; deprecated in group names? site_name = os.environ.get(\u0026#34;SITE\u0026#34;, None) if not site_name: print(\u0026#34;Define a site to query with SITE environment variable\u0026#34;) sys.exit(2) site_name = site_name.upper() if args.introspection: introspection_data = get_introspection_data(site_name) print(introspection_data) sys.exit(0) devices = get_devices(site_name) vms = get_vms(site_name) servers = devices + vms if args.list: hosts = generate_inventory(site_name, servers) print(hosts) elif args.host: pass ","permalink":"https://memooo.ooo/posts/pynetbox/","summary":"Usage:\npip install pynetbox ansible ansible all -i hosts/env -m setup --tree /tmp/facts/env #!/opt/netbox/bin/python import argparse import json import os import sys import pynetbox import yaml import urllib3 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) if sys.version_info \u0026lt; (3, 6): print(\u0026#34;Python 3.6 is required\u0026#34;) sys.exit(2) def to_json(in_dict): return json.dumps(in_dict, sort_keys=True, indent=4) def load_configuration(path=\u0026#34;/etc/ansible/netbox.yml\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Load netbox configuration /etc/ansible/netbox.yml \u0026#34;\u0026#34;\u0026#34; try: with open(path, \u0026#34;r\u0026#34;) as fd: return yaml.safe_load(fd) except yaml.YAMLError as yml_error: print(yml_error) NETBOX_ENDPOINT = load_configuration()[\u0026#34;netbox_endpoint\u0026#34;] NETBOX_TOKEN = load_configuration()[\u0026#34;netbox_token\u0026#34;] if not NETBOX_ENDPOINT: raise OSError(\u0026#34;environmet var NETBOX_ENDPOINT not set\u0026#34;) if not NETBOX_TOKEN: raise OSError(\u0026#34;environmet var NETBOX_TOKEN not set\u0026#34;) nb = pynetbox.","title":"Generating a dynamic host inventory for ansible with Netbox"},{"content":"The World from another point of view\nFun to imagine\nKnowing versus Understanding\n","permalink":"https://memooo.ooo/posts/richard-feynman/","summary":"The World from another point of view\nFun to imagine\nKnowing versus Understanding","title":"Some Richard Feynman videos"},{"content":"Waju or whatever is called is a game that until this day I don\u0026rsquo;t know from where it comes. But, is fun.\nEdit: The game is based on a German game called Mensch ärgere Dich nicht\nGoal The goal of the game is very simple:\nMove your marbles for one lap from your home to your goal. Don\u0026rsquo;t worry about winning but don\u0026rsquo;t let anyone else win. Have fun. Board The board has six main components:\nDice Shallow men believe in luck or in circumstance. Strong men believe in cause and effect. \u0026ndash;Ralph Waldo Emerson\nHome This is the waiting point and one of the most infuriating places you can be in the game, it\u0026rsquo;s also the place where you start.\nStart The starting point of your home is always located in the first spot to the right of your home.\nIn the picture above is the spot that has the same colour as the home or marbles in this case.\nRoad This is where the battle happens.\nVoid I\u0026rsquo;ve seen people staying here for months!!\nThe void is a shortcut you can take only from your starting point or your corner and only if you roll a 5 or 1 respectively.\nOnce in the void, you can only move back to the road by rolling a 1 and only a 1 to your corner to the right.\nRules Play the game with two dice. Put up to 4 marbles in your home. Each turn you roll two dice, and you get two movements For each die: If you roll 1 or 6, you roll again. And repeat as many times as you roll 1 or 6. Only reroll after you finish your available movements. You can spend a movement to take to the road a marble by rolling a 1 or 6 with any of your dice. If you don\u0026rsquo;t have any marble on the road and you don\u0026rsquo;t roll a 1 or 6 your turn pass. You can choose to spend the die in only the following ways:s Combine both dice and spend them in one marble. For instance, use the sum of two dice to move one marble forward. Spend one die in one marble and the second one in another marble. For instance, spend one movement to take a marble out of your home and the second one to move forward another marble. You cannot chose to spend two dice on the same marble in two movements on the same roll. If you reroll due to a 1 or 6 you can choose to move that marble again Taking out a marble from your home spends a movement and you *cannot move that marble again until your next turn or a reroll. If you don\u0026rsquo;t have any other marble to move, you can spend two separate movements in one marble. You cannot chose to spend only one die per roll. Only if you reroll one die due to 1 or 6. You can only move your marbles forward clockwise. You cannot jump or stay in the same spot as one of your marbles. If you put one of your marbles on the same spot as one of the other players, the other player\u0026rsquo;s marble must go to their respective home. Enjoy the feeling :) Bouncing back is possible, but must be your last choice Happens when the only option is to move the marble that will bounce. For instance, you should spend a movement to take a marble from home (if you have) to the road instead of bouncing. For each marble you have, move them forward on the road for one lap and place them in your goal spots. Other players cannot put their marbles on your goal spots. are you safe? You win if you put all your marbles on your goal. Schematics References https://en.m.wikipedia.org/wiki/Mensch_%C3%A4rgere_Dich_nicht ","permalink":"https://memooo.ooo/posts/waju/","summary":"Waju or whatever is called is a game that until this day I don\u0026rsquo;t know from where it comes. But, is fun.\nEdit: The game is based on a German game called Mensch ärgere Dich nicht\nGoal The goal of the game is very simple:\nMove your marbles for one lap from your home to your goal. Don\u0026rsquo;t worry about winning but don\u0026rsquo;t let anyone else win. Have fun. Board The board has six main components:","title":"Waju - A fun and brutal game"},{"content":" ","permalink":"https://memooo.ooo/posts/beyond-pep8/","summary":" ","title":"Beyond PEP 8 - Best practices for beautiful intelligible code"},{"content":"This laptop has very decent specs:\n8th Generation Intel® Core™ i7-8550U processor GPU: NVIDIA® GeForce® MX150 with 2 GB GDDR5 / Intel® UHD Graphics 620 16 GB LPDDR3 2133 MHz BT 4.1 (compatible with 3.0 and 2.1+EDR) 512 GB NVMe PCIe SSD Don\u0026rsquo;t expect running workstation level workloads in this machine but it is a wonderful dev machine.\nThings I don\u0026rsquo;t like about the laptop Palm rejection, especially this one, maybe this is Linux. Sound, it only outputs sound to two speakers under Linux and it has a werid noise under high volumes. BIOS configuration is too limited (but this is Huawei\u0026rsquo;s fault) Update 14/Jul/2019\nPalm rejection has improved a lot since I updated to the latest Touchpad software versions.\nDistro Ubuntu 18.04 with kernel 4.15.0-42-generic\nUpdate 14/Jul/2019\ndo-release-upgrade to Ubuntu 18.10 and upgraded kernel version to 5.0.0-050000-generic\nTouchpad sudo apt install acpi acpi-support acpica-tools acpid acpidump acpitail acpitool libacpi0 laptop-detect pommed xserver-xorg-input-synaptics Nvidia drivers sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update At this time, nvidia-driver-415 is the most up to date driver and the recommended one.\nsudo ubuntu-drivers autoinstall prime-select query For high-performance graphics, use:\nprime-select nvidia # log out and log in Verify nvidia is correctly installed:\nsudo lshw -C display glxinfo | grep OpenGL I\u0026rsquo;m getting readings about 12W to 17W battery discharge rate with this configuration.\nFor lower consumption, use:\nprime-select intel # log out and log in I\u0026rsquo;m getting readings about 4.5W to 6W battery discharge rate with this configuration.\nUpdate 14/Jul/2019\nAfter upgrading to kernel 5.0.0-050000-generic I\u0026rsquo;m getting discharge rates of 3.5W, not bad!!!\nDesktop Configuration i3wm sudo apt install i3wm i3lock vim ~/.config/i3/config\n# HDPI exec xrandr --dpi 220 # Applets exec --no-startup-id nm-applet exec --no-startup-id blueman-applet exec --no-startup-id gtk-redshift exec --no-startup-id megasync exec --no-startup-id dropbox start exec --no-startup-id flameshot # Lock screen bindsym $mod+l exec i3lock -c 000000 # background exec --no-startup-id /usr/bin/feh --randomize --bg-scale /path/wallpaper/* -Z And then, reload the configuration:\ni3-msg reload i3-msg restart Media keys For screen brightness and key backlights, I\u0026rsquo;m using Light\n# Sreen brightness controls bindsym XF86MonBrightnessUp exec light -A 5 bindsym XF86MonBrightnessDown exec light -U 5 # keyboard backlight controls bindsym XF86KbdBrightnessUp exec light -A 5 bindsym XF86KbdBrightnessDown exec light -A 5 For volume control, I\u0026rsquo;m using pactl\n# Volume controls bindsym XF86AudioLowerVolume exec /usr/bin/pactl set-sink-volume @DEFAULT_SINK@ \u0026#39;-5%\u0026#39; bindsym XF86AudioRaiseVolume exec /usr/bin/pactl set-sink-volume @DEFAULT_SINK@ \u0026#39;+5%\u0026#39; bindsym XF86AudioMute exec /usr/bin/pactl set-sink-mute @DEFAULT_SINK@ toggle Battery sudo apt install powertop tlp sudo powertop --calibrate sudo powertop --autotune sudo tlp start Disk I/O Following the graphical method steps on this webpage, I get the following speeds for my 512 GB NVMe PCIe SSD:\nAverage Read Rate: 1.4 GB/s (1000 samples) Average Write Read: 271.5 MB/s (1000 samples) Average Access Time: 0.11 msec (1000 samples) Maybe I\u0026rsquo;m testing it wrong, but it seems to me the write speeds are quite low.\nTroubleshooting Unsigned driver at boot If your Matebook X Pro does not boot after installing this nvidia driver or the one downloaded from nvidia\u0026rsquo;s website then disable the Secure Boot option in the BIOS.\nReconfigure the kernel sudo apt install --reinstall linux-image-generic linux-image-4.15.0-42-generic Remove old drivers sudo for FILE in $(dpkg-divert --list | grep nvidia-340 | awk \u0026#39;{print $3}\u0026#39;); do dpkg-divert --remove $FILE; done Using an eGPU in progress\nreferences https://github.com/ValveSoftware/steam-for-linux/issues/5707 https://wiki.ubuntu.com/UEFI/SecureBoot/Signing https://codeyarns.com/2013/02/07/how-to-fix-nvidia-driver-failure-on-ubuntu/ https://github.com/Syllo/nvtop https://askubuntu.com/questions/112705/how-do-i-make-powertop-changes-permanent https://int3ractive.com/2018/09/make-the-best-of-MacBook-touchpad-on-Ubuntu.html ","permalink":"https://memooo.ooo/posts/linux-matebook/","summary":"This laptop has very decent specs:\n8th Generation Intel® Core™ i7-8550U processor GPU: NVIDIA® GeForce® MX150 with 2 GB GDDR5 / Intel® UHD Graphics 620 16 GB LPDDR3 2133 MHz BT 4.1 (compatible with 3.0 and 2.1+EDR) 512 GB NVMe PCIe SSD Don\u0026rsquo;t expect running workstation level workloads in this machine but it is a wonderful dev machine.\nThings I don\u0026rsquo;t like about the laptop Palm rejection, especially this one, maybe this is Linux.","title":"Linux on Huawei Matebook X Pro"},{"content":"Note This is a Work-In-Progress Document and the most up-to-date information is available at: github.com/memogarcia/openstack-deployer\nDeploying OpenStack using containers allows easy customisation and flexibility on how to deploy the platform for development, testing and production environments.\nCurrent deployment: stable/queens\nHost configuration The default configuration for this environment is composed by 3 main components that need to run on the host:\nDocker Libvirtd OpenVSwitch Docker will act as the control plane for OpenStack while the host will provide the hypervisor, network and storage.\nNetwork topology This is the default network topology, 2 networks are used:\nopenstack-management-net: All openstack traffic goes through here openstack-provider-net: Instances get IPs in this network Infra services Fluentd: for logging Cadvisor: for container stats Elasticsearch: for log collection Kibana: for log visualization Portainer: for container management Third-party services Configure the third-party services needed for OpenStack to run.\nSeed MariaDB/MySQL PostgreSQL Optional Database Memcached Rabbitmq Onos Optional SDN Minio Optional Object Storage OpenStack services Keystone Glance Neutron Nova Nova-Qemu Cinder Horizon Extending OpenStack services Custom API Custom Backend Deploying OpenStack The model is a yml file describing how your environment should look like. It defines the services to run, networks, ips, volumes, dependencies, etc.\nConfigure your runtime environment by modifying model.yml.\nApply the configuration with config_processor, which will create the necessary scripts to run the environment.\nansible-playbook -i hosts/localhost config_processor.yml Config processor will create a new branch deploy where the runtime configuration will be ready for deployment.\nVerify the branch is created correctly.\ngit branch # * deploy git log # Ready for deployment Deploy OpenStack\n./scripts/docker-network-create.sh ./scripts/build.sh ./scripts/start.sh Verify installation source osrc-v3 openstack project list openstack image list openstack network list openstack server list References OpenStack installation Guide\n","permalink":"https://memooo.ooo/posts/openstack-containers/","summary":"Note This is a Work-In-Progress Document and the most up-to-date information is available at: github.com/memogarcia/openstack-deployer\nDeploying OpenStack using containers allows easy customisation and flexibility on how to deploy the platform for development, testing and production environments.\nCurrent deployment: stable/queens\nHost configuration The default configuration for this environment is composed by 3 main components that need to run on the host:\nDocker Libvirtd OpenVSwitch Docker will act as the control plane for OpenStack while the host will provide the hypervisor, network and storage.","title":"Deploying OpenStack with Docker"},{"content":"A Certificate Authority or CA is an entity that signs digital certificates. These digital certificates are used to validate the connection while using secure mechanisms.\nGenerating a root CA We will use a root CA to create intermediate CA\u0026rsquo;s which are trusted to sign certificates on its behalf.\nFirst, prepare the environment.\nmkdir /root/ca \u0026amp;\u0026amp; cd /root/ca mkdir certs crl newcerts private chmod 700 private touch index.txt echo 1000 \u0026gt; serial Then download the template for /root/ca/openssl.cnf from this gist and edit it.\nvim /root/ca/openssl.cnf Create the root key ca.key.pem and make sure to keep it secure.\nopenssl genrsa -aes256 -out private/ca.key.pem 4096 chmod 400 private/ca.key.pem Create a root certificate ca.cert.pem.\nopenssl req -config openssl.cnf \\ -key private/ca.key.pem \\ -new -x509 -days 10957 -sha256 -extensions v3_ca \\ -out certs/ca.cert.pem chmod 444 certs/ca.cert.pem Verify the root certificate.\nopenssl x509 -noout -text -in certs/ca.cert.pem Generating an intermediate CA It\u0026rsquo;s best practice to use intermediate CA\u0026rsquo;s instead of root CA\u0026rsquo;s to sign certificates, this practice allows a root CA to revoke a compromised intermediate CA and create a new one if necessary.\nPrepare the environment.\nmkdir /root/ca/intermediate \u0026amp;\u0026amp; cd /root/ca/intermediate mkdir certs crl csr newcerts private chmod 700 private touch index.txt echo 1000 \u0026gt; serial echo 1000 \u0026gt; /root/ca/intermediate/crlnumber Then download the template for /root/ca/intermediate/openssl.cnf from this gist and edit it.\nvim /root/ca/intermediate/openssl.cnf Create the intermediate key intermediate.key.pem.\ncd /root/ca openssl genrsa -aes256 \\ -out intermediate/private/intermediate.key.pem 4096 chmod 400 intermediate/private/intermediate.key.pem With the intermediate key create an intermediate certificate request intermediate.csr.pem for the root certificate to sign. Make sure that Common Name is different from your root CA\nopenssl req -config intermediate/openssl.cnf -new -sha256 \\ -key intermediate/private/intermediate.key.pem \\ -out intermediate/csr/intermediate.csr.pem The root CA will sign this certificate using v3_intermediate_ca extension. Make sure is valid for less time than the root CA\nopenssl ca -config openssl.cnf -extensions v3_intermediate_ca \\ -days 3650 -notext -md sha256 \\ -in intermediate/csr/intermediate.csr.pem \\ -out intermediate/certs/intermediate.cert.pem chmod 444 intermediate/certs/intermediate.cert.pem index.txt is the database file. Do NOT delete this file\nVeify the intermediate certificate.\nopenssl x509 -noout -text \\ -in intermediate/certs/intermediate.cert.pem Verify the intermediate CA against the root CA, the output should be OK.\nopenssl verify -CAfile certs/ca.cert.pem \\ intermediate/certs/intermediate.cert.pem After the verification is OK, chain the root CA and intermediate CA into a chain CA. This is only necessary if the root certificate is not installed on the client machines\ncat intermediate/certs/intermediate.cert.pem \\ certs/ca.cert.pem \u0026gt; intermediate/certs/ca-chain.cert.pem chmod 444 intermediate/certs/ca-chain.cert.pem Client certificates The intermediate certificate will be used to sign client certificates. Skip this step if you have a CSR already.\ncd /root/ca openssl genrsa -aes256 \\ -out intermediate/private/www.example.com.key.pem 2048 chmod 400 intermediate/private/www.example.com.key.pem Using 2048 bits for encryption on the client certificates is faster for TLS handshakes and lighter on the CPU but is less secure than using 4096 bits. Use it at discretion.\nUsing the private key intermediate/private/www.example.com.key.pem, create a CSR file. Skip this step if you have a CSR already.\nopenssl req -config intermediate/openssl.cnf \\ -key intermediate/private/www.example.com.key.pem \\ -new -sha256 -out intermediate/csr/www.example.com.csr.pem Signing client certificates To create a certificate, use the intermediate CA to sign the CSR.\nIf the certificate is going to use for:\nservers, use server_cert extension. authentication, use usr_cert extension. Usually, client certificates are valid for less time than the CA\u0026rsquo;s.\ncd /root/ca openssl ca -config intermediate/openssl.cnf \\ -extensions server_cert -days 375 -notext -md sha256 \\ -in intermediate/csr/www.example.com.csr.pem \\ -out intermediate/certs/www.example.com.cert.pem chmod 444 intermediate/certs/www.example.com.cert.pem Verification Verify that intermediate/index.txt contains a CN for your domain.\nVerify the certificate.\nopenssl x509 -noout -text \\ -in intermediate/certs/www.example.com.cert.pem Verify the CA certificate chain. the output should be OK.\nopenssl verify -CAfile intermediate/certs/ca-chain.cert.pem \\ intermediate/certs/www.example.com.cert.pem Distribution Distribute and/or deploy the following files:\n/root/ca/intermediate/certs/ca-chain.cert.pem /root/ca/intermediate/private/www.example.com.key.pem Only if you are signing the CSR /root/ca/intermediate/certs/www.example.com.cert.pem Next steps Sign certificates Cash in Sell out Bro down References OpenSSL Certificate Authority\n","permalink":"https://memooo.ooo/posts/becoming-ca/","summary":"A Certificate Authority or CA is an entity that signs digital certificates. These digital certificates are used to validate the connection while using secure mechanisms.\nGenerating a root CA We will use a root CA to create intermediate CA\u0026rsquo;s which are trusted to sign certificates on its behalf.\nFirst, prepare the environment.\nmkdir /root/ca \u0026amp;\u0026amp; cd /root/ca mkdir certs crl newcerts private chmod 700 private touch index.txt echo 1000 \u0026gt; serial Then download the template for /root/ca/openssl.","title":"Becoming a Certificate Authority (CA)"},{"content":"Note This is a Work-In-Progress Document.\nRead the docs at memogarcia/pratai-docs\nAbstract Pratai provides an incredibly flexible and resilient platform to migrate workloads to the cloud that respond to events without having to manage any server or network.\nHow it works The goal of Pratai is simple. Deploy \u0026ldquo;code\u0026rdquo; (disclaimer, from now on I will refer to code as functions), that will react to an event without worrying about anything else, the platform handles the execution. Simple right?\nIn order to achieve that, first, we need to deploy a function in a zip format for one of the languages that the platform supports, the first one is python but more will be added in the future, after this a docker image gets created with the custom function and the requirements. e.g.\n# new_module.py import numpy # yes you can install dependencies, just send a requirements.txt def local_function(payload): # you can create local functions return payload def main(payload=None): # a main function should always be declared # and using a payload as a parameter return local_function(payload) When a function gets created it will remain as inactive, waiting to be executed whenever an event happens that the function is subscribed to, could be a webhook endpoint, which can be assigned at creation time, or a message in a queue but basically, every event will spawn a container that will execute the event and then disappear.\nArchitecture Pratai is conformed of 2 major pieces, the Control Plane and the Nodes.\nControl Plane An API gateway, a database cluster and a load balancer, and agent and a scheduler runs in the control plane.\nFor the first version a API gateway built in python using flask will be made, in the future I think Golang should be a better option for it.\nAn elasticsearch cluster will power the storage of events, function metadata and cluster information.\nAnd a nginx load balancer will connect 3 instances to the API in a least_connect manner.\nPratai Nodes A Pratai node is composed by a driver and runtimes.\nWhen a new node is created it will automatically connect to the cluster and it will start polling for events.\nA driver is basically a container orchestrator like swarm, kubernetes, plain docker, etc. in this case we will use docker.\nThe runtimes are the languages supported by the platform, they are a base container image that contains an OS, a language and its dependencies, etc. that can be used by the functions the users submits. e.g.\n# seed/Dockerfile FROM ubuntu:14.04 RUN apt-get -y update RUN apt-get install -y git unzip wget # python27/Dockerfile FROM pratai/seed:latest RUN apt-get install -y python python-dev python-setuptools python-pip RUN pip install pip --upgrade # python27_template.txt FROM pratai/python27:latest RUN wget {zip_location} RUN unzip {zip_file} RUN pip install -r requirements.txt RUN mkdir /etc/pratai/ RUN mkdir /var/log/pratai/ RUN cp new_module.py /etc/pratai/ RUN git clone \u0026#34;repo_with_runtimes\u0026#34; CMD [\u0026#34;python\u0026#34;, \u0026#34;/pratai-runtimes/runtimes/python27/server.py\u0026#34;] Distributed Queues ZeroMQ is the choice for queuing and passing messages in pratai using the PUSH/PULL architecture we can create a pipelines of messages that can be distributed across multiple nodes.\nWe will have a producer and a collector running in the scheduler, and consumers running in the Pratai nodes, one consumer should be spawned per thread.\nEvents A function can react to any event coming through webhooks or messages in a queue, even events that happen in a database can trigger a function, is important to notice that a response of a function is an event, so it can trigger so chaining functions to build pipelines of data processing is easy with Pratai.\nThere are 2 kinds of events, async and wait_for_response\nAsync This is the default event for pratai, it will take a request or a message and process it asynchronously, then, you can collect the logs or responses, by default the response gets stored in a collector queue, that can send this response as an input for other functions.\nYou can use the async event in the following cases:\n1: Async + Webhook\nThis is the default behaviour, in which a function will be executed asynchronously when an HTTP POST requests hits its endpoint.\npratai function-create {name} --type async --event webhook 2: Async + Message\nA function created with this configuration will executed asynchronously when a message arrives in the event queues available for the platform.\npratai function-create {name} --type async --event message --subscribe_to {event_id} 3: Async + Timer\nA function created with this configuration will executed asynchronously every time a timer sends an event, the frequency of the events are set in minutes.\npratai function-create {name} --type async --event timer --frequency 5 Wait For Response This is a request that works like a typical web server, you send a request and you wait for a response and only works for webhooks events\npratai function-create {name} --type wait_for_response --event webhook Components API Gateway The API is the main interface for incoming webhook requests and for platform configuration.\nAgent The Agent is the main interface for events in queues and cron jobs.\nclient python-prataiclient is the component that allows the user to interact with the api from the command line interface, with it you can do stuff like this:\npratai function-create music_tag --file /path/to/zip --description \\ \u0026#34;extract metadata from music files\u0026#34; --memory 128 pratai function-list Because this is OpenStack you should pass credentials to interact with the platform\nexport OS_USERNAME=user export OS_PASSWORD=password export OS_TENANT_NAME=pratai_tenant export OS_PRATAI_URL=http://192.168.33.9:9096 export OS_IDENTITY_API_VERSION=3 export OS_AUTH_URL=http://192.168.33.9:5000/v3 export OS_PROJECT_NAME=pratai_tenant export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default Drivers A driver is a backend that orchestrate a container that contains the custom code.\nRuntimes A runtime is a language that is supported by the platform, it contains the language and its dependencies.\nScheduler The scheduler primarily consists of a set of Python daemons, though it requires and integrates with a number of native system components for databases and messaging capabilities.\n1; Scheduler\nThe API and the Agent push messages to this queue which will be pre-processed before being distributed among the pratai nodes.\n2; Collector\nWhen a function finish the function execution it will send the result and status here in order to be stored in the database afterwards.\nSecurity \u0026amp; Secrets The functions that interact with external services most often that not they require to use credentials to connect, for this, Barbican has been proposed to help with this scenario.\nWe definitely recommend using tokens instead of user/passwords when possible.\nCommunity Join us at #pratai irc channel in freenode\nRepositories memogarcia/pratai-docs memogarcia/pratai-agent memogarcia/pratai-api memogarcia/pratai-scheduler memogarcia/pratai-runtimes memogarcia/pratai-drivers References The Reactive Manifesto Cloud Design Patterns ","permalink":"https://memooo.ooo/posts/pratai/","summary":"Note This is a Work-In-Progress Document.\nRead the docs at memogarcia/pratai-docs\nAbstract Pratai provides an incredibly flexible and resilient platform to migrate workloads to the cloud that respond to events without having to manage any server or network.\nHow it works The goal of Pratai is simple. Deploy \u0026ldquo;code\u0026rdquo; (disclaimer, from now on I will refer to code as functions), that will react to an event without worrying about anything else, the platform handles the execution.","title":"Pratai, event driven platform for OpenStack"},{"content":"My blog is intended as a self reference and I don\u0026rsquo;t provide any support unless specified.\nIf you find anything useful, please どうぞ!\nLicense DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE Version 2, December 2004 Copyright (C) 2004 Sam Hocevar \u0026lt;sam@hocevar.net\u0026gt; Everyone is permitted to copy and distribute verbatim or modified copies of this license document, and changing it is allowed as long as the name is changed. DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION 0. You just DO WHAT THE FUCK YOU WANT TO. ","permalink":"https://memooo.ooo/about/","summary":"My blog is intended as a self reference and I don\u0026rsquo;t provide any support unless specified.\nIf you find anything useful, please どうぞ!\nLicense DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE Version 2, December 2004 Copyright (C) 2004 Sam Hocevar \u0026lt;sam@hocevar.net\u0026gt; Everyone is permitted to copy and distribute verbatim or modified copies of this license document, and changing it is allowed as long as the name is changed. DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION 0.","title":"About my blog"}]