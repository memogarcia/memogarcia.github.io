<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Behavior-driven development with LLMs | Memo Garcia</title>
<meta name=keywords content><meta name=description content="It is tempting to tell an LLM, “Build me a CRUD API in FastAPI,” and watch it do its thing. And sure, it’s amazing that:
LLMs can very vague requirements, and That we are able to “talk” to computers this way However, this tend to very a very imperative and not so repetable approach. This makes things “difficult” to predict and validate against an expected outcome.
It also makes difficult to test different LLMs on the same task to validate correctness, speed and price."><link rel=canonical href=https://memo.mx/posts/bdd-with-llms/><meta name=google-site-verification content="G-ZRB1GGCC9B"><link crossorigin=anonymous href=/assets/css/stylesheet.d1467e28595903f993a7e37893057cefe5f69aece461c850796bf5b0a3bf422a.css integrity="sha256-0UZ+KFlZA/mTp+N4kwV87+X2muzkYchQeWv1sKO/Qio=" rel="preload stylesheet" as=style><link rel=icon href=https://memo.mx/%20favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://memo.mx/%20favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://memo.mx/%20favicon-32x32.png><link rel=apple-touch-icon href=https://memo.mx/%20apple-touch-icon.png><link rel=mask-icon href=https://memo.mx/%20safari-pinned-tab.svg><meta name=theme-color content=" #2e2e33"><meta name=msapplication-TileColor content=" #2e2e33"><link rel=alternate hreflang=en href=https://memo.mx/posts/bdd-with-llms/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZRB1GGCC9B"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZRB1GGCC9B")}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://memo.mx/posts/"},{"@type":"ListItem","position":2,"name":"Behavior-driven development with LLMs","item":"https://memo.mx/posts/bdd-with-llms/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Behavior-driven development with LLMs","name":"Behavior-driven development with LLMs","description":"It is tempting to tell an LLM, “Build me a CRUD API in FastAPI,” and watch it do its thing. And sure, it’s amazing that:\nLLMs can very vague requirements, and That we are able to “talk” to computers this way However, this tend to very a very imperative and not so repetable approach. This makes things “difficult” to predict and validate against an expected outcome.\nIt also makes difficult to test different LLMs on the same task to validate correctness, speed and price.","keywords":[],"articleBody":"It is tempting to tell an LLM, “Build me a CRUD API in FastAPI,” and watch it do its thing. And sure, it’s amazing that:\nLLMs can very vague requirements, and That we are able to “talk” to computers this way However, this tend to very a very imperative and not so repetable approach. This makes things “difficult” to predict and validate against an expected outcome.\nIt also makes difficult to test different LLMs on the same task to validate correctness, speed and price.\nThis is where Behavior-Driven Development (BDD), The idea is to leverage it in order to gives us a structured, testable approach to prompting, this will help us to:\nBe precise on the outcome. Set clear requirements and constraints for the LLM to work with. Document the process in a declarative way. Creating a prompt plan using BDD A “prompt plan” is basically a blueprint for your LLM. For example:\nFeature: CRUD API to manage users As a system administrator, I want to manage users by performing create, read, update, and delete (CRUD) operations using a RESTful API, so that user data is efficiently managed. Context: Files to be added: - main.py - users.py Constraints: - Language: Python 3.13 - Libraries: httpx, fastapi, pydantic, uvicorn, tortoise-orm Scenario: Create a user Given an API endpoint for creating a user at \"/users\" When a user sends a POST request to the endpoint with valid user data: | Field | Value | | username | test_user | | email | test@test.com | | password | securePass123 | Then the API should return a response with: | Field | Value | | username | test_user | | email | test@test.com | And the user should be saved in the database When we write prompts like this, we give the LLM a clear target, making it easier to confirm whether the output matches our expectations.\nHow to use the prompt plan Just pass the plan to the LLM, if you are using Aider AI you can “copy-paste” the prompt plan directly into the terminal session.\nLLM limitations Besides the well known hallucination issues with LLMs there some huge limitations to them:\nContext size, Even with newer models offering larger “windows” for text, there’s still a limit to how much they can process at once. Context awareness, LLMs typically start fresh every time you prompt them. They don’t automatically know your entire codebase or environment. Unless you restate your requirements, they may miss critical details or propose solutions that conflict with your existing system. These limitations don’t make LLMs any less impressive—they just mean we need a solid plan, like BDD, to get consistent and testable results.\nOther tips Bad input, bad output Unless this is what you are looking for, don’t let LLMs assume stuff. Give as much detail and description of the outcome as possible. Set some personality to your LLMs by setting writing styles and tone of voice. ChatGPT is not the only game in town There are several options out there—self-hosted models, Claude, DeepSeek (depending on your situation).\nEditor Using something like VSCode with Copilot (or similar tools) places the LLM right in your development workflow. That can be more convenient than copying prompts into a separate tool.\nHave a clear goal in mind Methods like Test-Driven Development (TDD) and Behavior-Driven Development (BDD) can keep your LLM prompts focused. Know what you want to achieve from the start, then build the prompts around those specific outcomes.\n","wordCount":"577","inLanguage":"en","datePublished":"2025-01-22T00:03:30+01:00","dateModified":"2025-01-22T13:30:28+09:00","author":{"@type":"Person","name":"Memo Garcia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://memo.mx/posts/bdd-with-llms/"},"publisher":{"@type":"Organization","name":"Memo Garcia","logo":{"@type":"ImageObject","url":"https://memo.mx/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://memo.mx/ accesskey=h title="Memo Garcia (Alt + H)">Memo Garcia</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://memo.mx/dayjob title=$DAYJOB><span>$DAYJOB</span></a></li><li><a href=https://memo.mx/startup title=Startup><span>Startup</span></a></li><li><a href=https://memo.mx/youtube title=YouTube><span>YouTube</span></a></li><li><a href=https://memo.mx/books title=Books><span>Books</span></a></li><li><a href=https://memo.mx/about title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Behavior-driven development with LLMs</h1><div class=post-meta><span title='2025-01-22 00:03:30 +0100 +0100'>January 22, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Memo Garcia</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#creating-a-prompt-plan-using-bdd aria-label="Creating a prompt plan using BDD">Creating a prompt plan using BDD</a></li><li><a href=#how-to-use-the-prompt-plan aria-label="How to use the prompt plan">How to use the prompt plan</a></li><li><a href=#llm-limitations aria-label="LLM limitations">LLM limitations</a></li><li><a href=#other-tips aria-label="Other tips">Other tips</a><ul><li><a href=#bad-input-bad-output aria-label="Bad input, bad output">Bad input, bad output</a></li><li><a href=#chatgpt-is-not-the-only-game-in-town aria-label="ChatGPT is not the only game in town">ChatGPT is not the only game in town</a></li><li><a href=#editor aria-label=Editor>Editor</a></li><li><a href=#have-a-clear-goal-in-mind aria-label="Have a clear goal in mind">Have a clear goal in mind</a></li></ul></li></ul></div></details></div><div class=post-content><p>It is tempting to tell an LLM, “Build me a CRUD API in FastAPI,” and watch it do its thing. And sure, it’s amazing that:</p><ol><li>LLMs can very vague requirements, and</li><li>That we are able to “talk” to computers this way</li></ol><p>However, this tend to very a very imperative and not so repetable approach. This makes things “difficult” to predict and validate against an expected outcome.</p><p>It also makes difficult to test different LLMs on the same task to validate correctness, speed and price.</p><p>This is where <strong>Behavior-Driven Development (BDD)</strong>, The idea is to leverage it in order to gives us a structured, testable approach to prompting, this will help us to:</p><ol><li>Be precise on the outcome.</li><li>Set clear requirements and constraints for the LLM to work with.</li><li>Document the process in a declarative way.</li></ol><h2 id=creating-a-prompt-plan-using-bdd>Creating a prompt plan using BDD<a hidden class=anchor aria-hidden=true href=#creating-a-prompt-plan-using-bdd>#</a></h2><p>A “prompt plan” is basically a blueprint for your LLM. For example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Feature: CRUD API to manage users
</span></span><span class=line><span class=cl>  As a system administrator, I want to manage users by performing create, read, update, and delete <span class=o>(</span>CRUD<span class=o>)</span> operations
</span></span><span class=line><span class=cl>  using a RESTful API, so that user data is efficiently managed.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  Context:
</span></span><span class=line><span class=cl>    Files to be added:
</span></span><span class=line><span class=cl>      - main.py
</span></span><span class=line><span class=cl>      - users.py
</span></span><span class=line><span class=cl>    Constraints:
</span></span><span class=line><span class=cl>      - Language: Python 3.13
</span></span><span class=line><span class=cl>      - Libraries: httpx, fastapi, pydantic, uvicorn, tortoise-orm
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  Scenario: Create a user
</span></span><span class=line><span class=cl>    Given an API endpoint <span class=k>for</span> creating a user at <span class=s2>&#34;/users&#34;</span>
</span></span><span class=line><span class=cl>    When a user sends a POST request to the endpoint with valid user data:
</span></span><span class=line><span class=cl>      <span class=p>|</span> Field      <span class=p>|</span> Value       <span class=p>|</span>
</span></span><span class=line><span class=cl>      <span class=p>|</span> username   <span class=p>|</span> test_user   <span class=p>|</span>
</span></span><span class=line><span class=cl>      <span class=p>|</span> email      <span class=p>|</span> test@test.com <span class=p>|</span>
</span></span><span class=line><span class=cl>      <span class=p>|</span> password   <span class=p>|</span> securePass123 <span class=p>|</span>
</span></span><span class=line><span class=cl>    Then the API should <span class=k>return</span> a response with:
</span></span><span class=line><span class=cl>      <span class=p>|</span> Field      <span class=p>|</span> Value       <span class=p>|</span>
</span></span><span class=line><span class=cl>      <span class=p>|</span> username   <span class=p>|</span> test_user   <span class=p>|</span>
</span></span><span class=line><span class=cl>      <span class=p>|</span> email      <span class=p>|</span> test@test.com <span class=p>|</span>
</span></span><span class=line><span class=cl>    And the user should be saved in the database
</span></span></code></pre></div><p>When we write prompts like this, we give the LLM a clear target, making it easier to confirm whether the output matches our expectations.</p><h2 id=how-to-use-the-prompt-plan>How to use the prompt plan<a hidden class=anchor aria-hidden=true href=#how-to-use-the-prompt-plan>#</a></h2><p>Just pass the plan to the LLM, if you are using <a href=https://aider.chat>Aider AI</a> you can &ldquo;copy-paste&rdquo; the <code>prompt plan</code> directly into the terminal session.</p><h2 id=llm-limitations>LLM limitations<a hidden class=anchor aria-hidden=true href=#llm-limitations>#</a></h2><p>Besides the well known hallucination issues with LLMs there some huge limitations to them:</p><ol><li>Context size, Even with newer models offering larger “windows” for text, there’s still a limit to how much they can process at once.</li><li>Context awareness, LLMs typically start fresh every time you prompt them. They don’t automatically know your entire codebase or environment. Unless you restate your requirements, they may miss critical details or propose solutions that conflict with your existing system.</li></ol><p>These limitations don’t make LLMs any less impressive—they just mean we need a solid plan, like BDD, to get consistent and testable results.</p><h2 id=other-tips>Other tips<a hidden class=anchor aria-hidden=true href=#other-tips>#</a></h2><h3 id=bad-input-bad-output>Bad input, bad output<a hidden class=anchor aria-hidden=true href=#bad-input-bad-output>#</a></h3><ol><li>Unless this is what you are looking for, don’t let LLMs assume stuff.</li><li>Give as much detail and description of the outcome as possible.</li><li>Set some personality to your LLMs by setting writing styles and tone of voice.</li></ol><h3 id=chatgpt-is-not-the-only-game-in-town>ChatGPT is not the only game in town<a hidden class=anchor aria-hidden=true href=#chatgpt-is-not-the-only-game-in-town>#</a></h3><p>There are several options out there—self-hosted models, Claude, DeepSeek (depending on your situation).</p><h3 id=editor>Editor<a hidden class=anchor aria-hidden=true href=#editor>#</a></h3><p>Using something like VSCode with Copilot (or similar tools) places the LLM right in your development workflow. That can be more convenient than copying prompts into a separate tool.</p><h3 id=have-a-clear-goal-in-mind>Have a clear goal in mind<a hidden class=anchor aria-hidden=true href=#have-a-clear-goal-in-mind>#</a></h3><p>Methods like Test-Driven Development (TDD) and Behavior-Driven Development (BDD) can keep your LLM prompts focused. Know what you want to achieve from the start, then build the prompts around those specific outcomes.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://memo.mx/posts/happiness/><span class=title>Next »</span><br><span>Happiness</span></a></nav></footer></article></main><footer class=footer></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>